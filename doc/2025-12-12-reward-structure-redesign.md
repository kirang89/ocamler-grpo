# GRPO Reward Structure Redesign
## Removing END Markers & Strengthening Graduated Rewards

**Date:** 2025-12-12
**Status:** üü° **IMPLEMENTED - PENDING VALIDATION**
**Previous Context:** [Training Failure Analysis (Epoch 0.21)](./2025-12-11-epoch-0.21-training-failure-analysis.md)

---

## Executive Summary

After reviewing research on GRPO/RLVR best practices and analyzing the END marker gaming issue, we implemented a **simplified, research-backed reward design** that:

1. ‚úÖ **Removed END marker** constraint (eliminated gaming vector)
2. ‚úÖ **Strengthened graduated type checking rewards** (25% with larger gaps: 0.02 ‚Üí 0.05 ‚Üí 0.10 ‚Üí 0.15 ‚Üí 0.20 ‚Üí 0.25)
3. ‚úÖ **Added multi-signal prose detection** (4 independent signals, requires 2+ to trigger)
4. ‚úÖ **Weakened prose penalty** (0.3 multiplier instead of 0.05 - guardrail not driver)
5. ‚úÖ **Added entropy-based token filtering** (`top_entropy_quantile=0.2` - focus on uncertain tokens)

**Key Insight:** The previous report recommended adding complexity (entropy bonus, NL penalty, mode collapse detection). Research revealed that **TRL doesn't support entropy bonus**, and that **strengthening positive rewards is more important than penalizing negatives**.

**Design Philosophy:**
- Graduated rewards = primary learning signal
- Prose penalty = secondary guardrail
- Avoid reward hacking by making real attempts more valuable than pattern avoidance

---

## Research Findings

### 1. **TRL GRPO Does NOT Support Entropy Bonus**

**Critical Discovery:** Despite entropy bonus being recommended in the previous report, TRL's `GRPOConfig` does **not** have parameters to add entropy as a reward component.

**What TRL DOES support:**
- `top_entropy_quantile`: Filters training to focus on high-entropy tokens (efficiency optimization)
- NOT the same as adding entropy as a reward bonus

**Sources:**
- [TRL v0.20.0 Release Notes](https://github.com/huggingface/trl/releases/tag/v0.20.0)
- [TRL GRPOConfig Documentation](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_config.py)

### 2. **Entropy Regularization May Not Work with GRPO**

**Research finding:**
> "GRPO suffers from entropy collapse, where entropy monotonically decreases, exploration vanishes, and policies converge prematurely. **Existing entropy-regularized methods only partially alleviate this issue while introducing bias and instability.**"
>
> "**Entropy regularization is not a good fit for the critic-free GRPO setting.**"

**Source:** [SPINE: Token-Selective Test-Time Reinforcement Learning](https://arxiv.org/html/2511.17938)

**Implication:** Simple entropy bonuses might not solve mode collapse and could introduce training instability.

### 3. **What Actually Works: Advanced Entropy Methods**

Successful approaches use sophisticated entropy handling:

| Method | Approach | Status |
|--------|----------|--------|
| **GTPO/GRPO-S** | Dynamic entropy weighting per token | Requires custom implementation |
| **EDGE-GRPO** | Entropy-driven advantage diversity | Not in TRL |
| **AEPO** | Temperature regulation instead of bonuses | Not in TRL |
| **Pass@k Training** | Advantage shaping from groups | Requires custom implementation |

**Source:** [GTPO and GRPO-S](https://arxiv.org/html/2508.04349v1)

### 4. **DeepSeek's Actual Approach (Not Pure Binary)**

**Claim:** DeepSeek uses binary (0/1) rewards for RLVR.

**Reality:** DeepSeek uses **two types of rewards**:
- **Accuracy Rewards:** Binary (0/1) for correct final answer
- **Format Rewards:** Graduated rewards for structured output (e.g., `<think>...</think>` tags)

**Critical Analysis:**
> "Where does the relative advantage come from when all incorrect answers receive the same reward? GRPO updates models based on **relative ranking** of responses. If every wrong answer gets 0 reward, there's **no meaningful way to compare them**."

**Hypothesis:** DeepSeek likely uses hidden partial credit mechanisms not disclosed in their paper.

**Source:** [DeepSeek's Lies: A Closer Look at GRPO Implementation](https://medium.com/intelligence-factory/deepseeks-lies-a-closer-look-at-grpo-implementation-dea4607842e9)

### 5. **END Marker Usage in Production**

**Finding:** Modern GRPO implementations rely on **native EOS tokens**, not custom markers like `(* END *)`.

**Standard practice:**
- Use model's built-in EOS token (e.g., `<|endoftext|>`)
- Create binary masks for tokens after first EOS
- Monitor `clipped_ratio` for truncations (completions hitting max_length)

**Problem with custom markers:**
> "Structural/format tokens can be **unintentionally penalized during training**, degrading formatting and stability over time."

Models get stuck in "redundant verification loops" or spam patterns (exactly what we observed with BEGIN/END spam).

**Source:** [RLHF Book - Policy Gradient Algorithms](https://rlhfbook.com/c/11-policy-gradients.html)

---

## Problem Analysis

### The END Marker Gaming Issue

From the previous report, we observed **5 degenerate patterns**:

1. ‚úÖ **BEGIN/END spam** ‚Üí Directly caused by END marker reward
2. ‚ùå **Natural language** ‚Üí Instruction tuning contamination
3. ‚ùå **Repetitive code blocks** ‚Üí Low-effort strategy
4. ‚ùå **Minimal junk** ‚Üí Quick termination
5. ‚ùå **Random text** ‚Üí Model is lost

**END marker only directly causes 1/5 patterns**, but contributes to the problem:
- Adds artificial constraint for model to learn
- Creates gaming opportunity (+0.05 reward for marker)
- Research shows custom markers degrade training stability

### The Reward Hacking Risk

**Concern:** With strong prose penalties, could the model learn to avoid prose patterns without learning to code?

**Scenario:**
```
Strategy A (Real Learning):
  Try to write correct OCaml
  ‚Üí 98% fail (0.02-0.20), 2% succeed (1.0)
  ‚Üí Expected Value: 0.024

Strategy B (Reward Hacking):
  Generate OCaml-looking gibberish that avoids prose patterns
  ‚Üí 100% fail code (0.10-0.15), but no prose penalty
  ‚Üí Expected Value: 0.125

Strategy B has 5x higher EV! ‚Üê Reward hacking wins
```

**Solution:** Make Strategy A more attractive by:
1. Strengthening graduated rewards (increase partial credit)
2. Weakening prose penalty (guardrail, not dominant signal)
3. Using multi-signal detection (harder to game)

---

## Implementation Details

### Change 1: Remove END Marker

**Files Modified:** `train.py` (lines 54-104, 438-499, 551-688)

**What was removed:**
```python
# Removed from prompt
"and end the response with the exact marker `(* END *)`. Do not emit any prose, explanations, or trailing text after the marker."

# Removed from examples
(* END *)  # Removed from all 3 examples

# Removed constants
END_MARKER = "(* END *)"
RUNAWAY_PENALTY_MULTIPLIER = 0.3

# Removed functions
def score_has_end_marker(completion: str) -> float:
    return 1.0 if completion.strip().endswith(END_MARKER) else 0.0

# Removed from reward calculation
structural_score = 0.0
if completion.strip().endswith(END_MARKER):
    structural_score += 0.05  # ‚Üê Removed

# Removed runaway penalty
is_runaway = len(completion) >= 500 and not completion.strip().endswith(END_MARKER)
if is_runaway:
    total_reward = 0.0  # ‚Üê Removed
```

**Rationale:**
- Eliminates gaming vector (BEGIN/END spam pattern)
- Aligns with research-backed best practices (use native EOS)
- Simplifies prompt (one less constraint to learn)
- Model now relies on natural termination or max_length

### Change 2: Add Multi-Signal Prose Detection

**Files Modified:** `train.py` (lines 438-499)

**New function:**
```python
def is_degenerate_output(completion: str, code: str) -> bool:
    """
    Multi-signal detection for degenerate outputs (prose, gibberish, spam).
    Returns True if output appears degenerate. Requires 2+ signals to avoid false positives.
    """
    issues = 0

    # Signal 1: Conversational prose patterns
    PROSE_PATTERNS = [
        r"To solve this",
        r"Here'?s",
        r"I apologize",
        r"Let me",
        r"You can use",
        r"The solution",
        r"This (approach|implementation|works|method)",
        r"[.!?]\s+[A-Z]",  # Multiple sentences
    ]
    for pattern in PROSE_PATTERNS:
        if re.search(pattern, completion, re.IGNORECASE):
            issues += 1
            break

    # Signal 2: Low OCaml keyword density (gibberish)
    keywords = len(re.findall(
        r'\b(let|match|with|if|then|else|fun|rec|type|val|module|open|in)\b',
        code
    ))
    keyword_density = keywords / len(code.split()) if code else 0
    if keyword_density < 0.05:  # Real OCaml has ~10-20%
        issues += 1

    # Signal 3: High repetition (spam)
    chunks = [completion[i:i+50] for i in range(0, len(completion)-50, 25)]
    if chunks:
        repetition_ratio = len(set(chunks)) / len(chunks)
        if repetition_ratio < 0.3:  # >70% repetition
            issues += 1

    # Signal 4: Low code purity (too much wrapper)
    if len(code) > 0 and len(completion) > 0:
        code_purity = len(code) / len(completion)
        if code_purity < 0.5:  # Less than half is code
            issues += 1

    # Require 2+ signals (prevents false positives)
    return issues >= 2
```

**Why multi-signal:**
- Single regex patterns easy to game ("avoid 'To solve'")
- Must fake multiple independent signals simultaneously
- More expensive to game than to learn the task
- Reduces false positives on legitimate OCaml comments/docstrings

### Change 3: Strengthen Graduated Type Checking Rewards

**Files Modified:** `train.py` (lines 597-623)

**Old rewards (sparse):**
```python
0 errors: 0.20 (100%)
1 error:  0.15 (75%)   # Gap: 0.05
2 errors: 0.10 (50%)   # Gap: 0.05
3 errors: 0.06 (30%)   # Gap: 0.04
4 errors: 0.03 (15%)   # Gap: 0.03
5+ errors: 0.02 (10%)  # Gap: 0.01
```

**New rewards (stronger gradient):**
```python
0 errors: 0.25 (100%)  # +25% increase
1 error:  0.20 (80%)   # Gap: 0.05, +33% increase
2 errors: 0.15 (60%)   # Gap: 0.05, +50% increase
3 errors: 0.10 (40%)   # Gap: 0.05, +67% increase
4 errors: 0.05 (20%)   # Gap: 0.05, +67% increase
5+ errors: 0.02 (8%)   # Gap: 0.03, same floor
```

**Key improvements:**
- **Consistent 0.05 gaps** between most levels (easier to climb gradient)
- **Higher absolute values** (0.20‚Üí0.25, 0.15‚Üí0.20, etc.)
- **Larger relative increases** for near-perfect code (1-2 errors get 33-50% more)
- **Floor remains low** (0.02) to distinguish gibberish from real attempts

**Why this matters:**
```
Scenario: Model with 98% failure rate

Old system:
- Gibberish: 0.02
- 4 errors: 0.03  ‚Üê Only 0.01 difference!
- 3 errors: 0.06  ‚Üê Hard to see gradient

New system:
- Gibberish: 0.02
- 4 errors: 0.05  ‚Üê 2.5x better than gibberish
- 3 errors: 0.10  ‚Üê 2x better than 4 errors
- 2 errors: 0.15  ‚Üê 1.5x better than 3 errors

Clearer gradient = stronger learning signal
```

### Change 4: Weaken Prose Penalty

**Files Modified:** `train.py` (lines 678-686)

**Old penalty (dominant):**
```python
if has_prose_patterns(completion):
    total_reward *= 0.05  # 95% penalty
```

**New penalty (guardrail):**
```python
if is_degenerate_output(completion, code):
    total_reward *= 0.3  # 70% penalty
```

**Impact comparison:**

| Scenario | Old Reward | New Reward | Winner |
|----------|-----------|-----------|--------|
| Perfect code, no prose | 1.00 | 1.00 | Tie |
| Perfect code, with prose | 0.05 | 0.30 | New (6x better!) |
| Near-working (0.20), no prose | 0.20 | 0.20 | Tie |
| Near-working (0.20), with prose | 0.01 | 0.06 | New (6x better!) |
| Gibberish (0.05), no prose | 0.05 | 0.05 | Tie |
| Gibberish (0.05), with prose | 0.0025 | 0.015 | Both terrible |

**Why this prevents reward hacking:**

**Old system (prose penalty dominant):**
```
Real code attempt (0.20) with prose ‚Üí 0.01 reward
Gibberish (0.05) without prose ‚Üí 0.05 reward
Model learns: "Avoiding prose > writing code" ‚Üê WRONG!
```

**New system (graduated rewards dominant):**
```
Real code attempt (0.20) with prose ‚Üí 0.06 reward
Gibberish (0.05) without prose ‚Üí 0.05 reward
Model learns: "Writing better code > avoiding prose" ‚Üê CORRECT!
```

### Change 5: Add Entropy-Based Token Filtering

**Files Modified:** `train.py` (lines 771-801)

**New configuration:**
```python
# Optional: Entropy-based token filtering (focuses training on high-entropy tokens)
# Based on "Beyond the 80/20 Rule" paper - using 20% of highest entropy tokens
# achieves similar performance to all tokens while improving efficiency
top_entropy_quantile = float(os.environ.get("GRPO_TOP_ENTROPY_QUANTILE", "0.2"))

return GRPOConfig(
    # ... other params
    top_entropy_quantile=top_entropy_quantile,  # Focus on uncertain tokens
)
```

**What this does:**
- Filters training to focus on top 20% most uncertain (high-entropy) tokens
- Low-entropy tokens (model very confident) ignored for training
- Improves efficiency without sacrificing performance

**Example:**
```ocaml
let rec factorial n = if n = 0 then 1 else n * factorial (n-1)

Low entropy (ignored):  "let", "=", "then", "else" (grammar/syntax)
High entropy (trained): "rec" vs "inline", "factorial" vs "helper", "0" vs "1" (logic)
```

**Benefits:**
- 80% speedup (only train on 20% of tokens)
- Focus learning on algorithmic decisions vs syntax
- Research shows no loss in final performance

**Source:** [Beyond the 80/20 Rule Paper](https://huggingface.co/docs/trl/main/en/grpo_trainer)

---

## New Reward Structure

### Complete Breakdown

| Component | Max Reward | Previous | Change | Type |
|-----------|-----------|----------|--------|------|
| **Structural (END marker)** | ‚Äî | 5% | **Removed** | ‚Äî |
| **Type checking** | 25% | 20% | **+5%** | Graduated (6 levels) |
| **Compilation** | 10% | 10% | Same | Graduated (3 levels) |
| **Tests** | 65% | 65% | Same | All-or-nothing* |
| **Prose penalty** | √ó0.3 | √ó0.05 | **Weakened 6x** | Multiplier |

*Note: Test structure allows for future enhancement to count partial test passes

### Reward Ranges by Outcome

| Outcome | Range | Example Scenarios |
|---------|-------|-------------------|
| **Perfect** | 1.00 | All tests pass, no prose |
| **Very Good** | 0.35-0.65 | Compiles, some tests pass |
| **Good Progress** | 0.20-0.35 | 1-2 type errors, no compile |
| **Some Progress** | 0.10-0.20 | 3-4 type errors |
| **Minimal Effort** | 0.02-0.05 | 5+ errors or gibberish |
| **Degenerate + Prose** | 0.006-0.015 | Gibberish with prose (70% penalty) |

### Expected Value Comparison

**With 98% failure rate:**

| Strategy | Old EV | New EV | Winner |
|----------|--------|--------|--------|
| Try hard, real code attempts | 0.024 | **0.040** | **New (67% better)** |
| Avoid prose, write gibberish | 0.05 | 0.05 | Tie |
| Write prose + gibberish | 0.0025 | 0.015 | Both terrible |

**New system makes real attempts 67% more valuable despite high failure rate!**

---

## Expected Outcomes

### Positive Changes Expected

1. **Stronger Learning Gradient**
   - Larger gaps in type checking (0.05 per level vs inconsistent)
   - Clearer signal when model improves from 4‚Üí3‚Üí2‚Üí1 errors
   - Higher absolute rewards make progress more rewarding

2. **Better Exploration vs Exploitation Balance**
   - Prose penalty won't dominate signal
   - Model incentivized to try different code approaches
   - Real attempts (even with some prose) beat pure pattern avoidance

3. **Reduced Mode Collapse Risk**
   - Multi-signal detection harder to game than simple regex
   - Must fake 2+ independent signals to hack penalty
   - Graduated rewards provide continuous learning signal

4. **Simpler Prompt**
   - No artificial END marker constraint
   - Model can focus on learning OCaml, not marker placement
   - Aligns with standard GRPO practices

5. **Improved Efficiency**
   - Entropy filtering focuses on important tokens
   - 80% speedup in token processing
   - Research shows no performance loss

### Risks & Mitigations

#### Risk 1: Model Never Learns to Stop

**Risk:** Without END marker, completions always hit max_length (600 chars).

**Mitigation:**
- Model has native EOS token (`<|endoftext|>`)
- If completion hits max_length, likely truncated (bad code anyway)
- Can monitor `clipped_ratio` metric for truncation rate
- Can add length-based penalty if needed: `if len > 550: reward *= 0.5`

**Likelihood:** Low (model knows EOS from pretraining)

#### Risk 2: Prose Penalty Too Weak

**Risk:** Model generates prose-wrapped code, gets decent reward.

**Mitigation:**
- Multi-signal detection catches multiple issues
- Prose + any other issue (repetition/low purity) = penalty
- Can strengthen penalty (0.3 ‚Üí 0.2) if problem persists
- Monitor `prose_penalty_applied` frequency in logs

**Likelihood:** Medium (but acceptable - prose-wrapped correct code still valuable)

#### Risk 3: Reward Hacking Still Occurs

**Risk:** Model finds new degenerate pattern we didn't anticipate.

**Mitigation:**
- Graduated rewards provide strong positive signal
- Multi-signal detection covers multiple degenerate types
- Can add new signals to `is_degenerate_output()` if needed
- Regular monitoring of completions.jsonl for new patterns

**Likelihood:** Medium (mode collapse is persistent with 98% failure rate)

#### Risk 4: Strengthened Rewards Not Enough

**Risk:** Even with stronger gradient, 98% failure rate still causes collapse.

**Contingency Plan:**
1. Implement curriculum learning (easy problems first)
2. Consider custom GTPO/GRPO-S entropy weighting
3. Explore Pass@k training with advantage shaping
4. Switch to non-instruct base model + SFT initialization

**Likelihood:** Medium (sparse rewards remain fundamental issue)

---

## Validation Plan

### Metrics to Monitor

**Primary Success Metrics** (in `grpo_runs/learning.log`):

| Metric | Healthy | Warning | Critical | Action |
|--------|---------|---------|----------|--------|
| `reward_mean` | > 0.10 | 0.05-0.10 | < 0.03 | Investigate collapse |
| `entropy` | > 0.30 | 0.15-0.30 | < 0.08 | Check for deterministic outputs |
| `frac_zero_std` | 0.0-0.25 | 0.25-0.50 | > 0.75 | Stop training (collapse) |
| `grad_norm` | > 0.05 | 0.01-0.05 | < 0.005 | Check for zero gradients |

**Secondary Metrics:**

| Metric | Target | Purpose |
|--------|--------|---------|
| `loss` | Gradual decrease | Learning progress |
| `reward_std` | > 0.05 | Diversity in rewards |
| `learning_rate` | 1e-6 | Stability |

### Completion Quality Checks

**Every 50 steps, sample 5 completions from `completions.jsonl` and verify:**

‚úÖ **Good Signs:**
- Pure OCaml code (no markdown fences or prose)
- Variety in approaches across samples
- Attempts at actual solutions (not random text)
- No repetitive patterns (spam)
- Proper OCaml syntax structure

üö® **Bad Signs:**
- Natural language explanations returning
- Repetitive spam patterns (new forms)
- Identical completions across samples (mode collapse)
- Random gibberish text
- Hitting max_length every time (truncation)

### Comparison Baselines

**Compare against previous training (Epoch 0.20-0.21):**

| Metric | Previous | Target | Status |
|--------|----------|--------|--------|
| `reward_mean` | 0.018-0.044 | > 0.10 | TBD |
| `entropy` | 0.090-0.728 (unstable) | > 0.15 (stable) | TBD |
| `frac_zero_std` | 0.75 (critical) | < 0.50 | TBD |
| Success rate | ~2% | > 5% | TBD |
| Prose completions | ~20% | < 10% | TBD |

### Test Run Schedule

**Phase 1: Quick Validation (100 steps, ~30 min)**
```bash
GRPO_NUM_EPOCHS=0.02 uv run train.py > test-run.log 2>&1
tail -f grpo_runs/learning.log
```

**Success criteria:**
- No immediate crashes or errors
- Rewards not stuck at 0.0
- Entropy not immediately collapsing
- Some variety in completions

**Phase 2: Short Training (500 steps, ~2 hours)**
```bash
GRPO_NUM_EPOCHS=0.1 uv run train.py > short-run.log 2>&1
```

**Success criteria:**
- `frac_zero_std < 0.50` throughout
- `entropy > 0.15` maintained
- `reward_mean > 0.05` by end
- < 20% prose completions

**Phase 3: Full Training (If Phase 2 succeeds)**
```bash
nohup uv run train.py > training-full.log 2>&1 &
```

**Success criteria:**
- Gradual reward improvement (0.05 ‚Üí 0.10 ‚Üí 0.15)
- Stable entropy (no collapse)
- Success rate 2% ‚Üí 5-10%
- Model converges to useful behavior

---

## Comparison to Previous Report Recommendations

### What We Implemented

| Recommendation | Status | Notes |
|----------------|--------|-------|
| Remove END marker | ‚úÖ **Done** | Aligns with research |
| Strengthen graduated rewards | ‚úÖ **Done** | Type checking 20%‚Üí25% with larger gaps |
| Multi-signal prose detection | ‚úÖ **Done** | 4 signals, requires 2+ to trigger |
| Weaken prose penalty | ‚úÖ **Done** | 0.05‚Üí0.3 (guardrail not driver) |
| Entropy filtering | ‚úÖ **Done** | `top_entropy_quantile=0.2` |

### What We Did NOT Implement (And Why)

| Recommendation | Status | Reason |
|----------------|--------|--------|
| Entropy bonus (0.02 * entropy) | ‚ùå **Not possible** | TRL doesn't support, may cause instability |
| Mode collapse auto-stop callback | ‚è∏Ô∏è **Deferred** | Implement if collapse persists |
| Partial test credit | ‚è∏Ô∏è **Deferred** | Requires parsing OCaml test output |
| Curriculum learning | ‚è∏Ô∏è **Deferred** | Implement if validation fails |
| Switch to non-instruct model | ‚è∏Ô∏è **Contingency** | Last resort if all else fails |

### Key Differences in Approach

**Previous Report:**
- Add complexity (entropy bonus, NL penalty, auto-stop)
- Focus on preventing bad behavior (penalties)
- Multiple moving parts

**This Implementation:**
- Simplify design (remove END marker)
- Focus on rewarding good behavior (stronger positive rewards)
- Research-backed best practices

**Philosophy shift:**
> "Don't tell the model what NOT to do (prose, spam, etc.).
> Tell the model what TO do (write better OCaml) and make that rewarding."

---

## Next Steps

### Immediate (Today)

1. ‚úÖ **Implementation complete** - All changes merged to `train.py`
2. ‚è≥ **Phase 1: Quick validation** - 100 steps to check no immediate issues
3. ‚è≥ **Phase 2: Short training** - 500 steps to validate approach

### If Validation Succeeds

4. Run full training (1 epoch)
5. Monitor metrics dashboard every 2-4 hours
6. Sample completions.jsonl every 100 steps
7. Compare final model to baseline (evaluate.py)

### If Mode Collapse Persists

**Tier 1 (Easy):**
- Strengthen type checking further (25% ‚Üí 30%)
- Adjust prose penalty (0.3 ‚Üí 0.2 or 0.4)
- Add length-based penalty for truncations

**Tier 2 (Medium):**
- Implement mode collapse auto-stop callback
- Add partial test credit (parse OCaml assertions)
- Experiment with higher temperature (1.0 ‚Üí 1.2)

**Tier 3 (Hard):**
- Implement custom GTPO/GRPO-S entropy weighting
- Implement Pass@k training with advantage shaping
- Curriculum learning (easy problems ‚Üí hard problems)

**Tier 4 (Contingency):**
- Switch to non-instruct base model
- SFT initialization phase before GRPO
- Reconsider dataset difficulty

---

## Code Changes Summary

### Files Modified

- **`train.py`**: 200+ lines changed
  - Removed END marker logic (prompt, constants, functions, rewards)
  - Added `is_degenerate_output()` function (60 lines)
  - Strengthened type checking rewards (6 values updated)
  - Weakened prose penalty (0.05 ‚Üí 0.3)
  - Added entropy filtering config
  - Updated logging (removed structural, added prose penalty tracking)

### Lines of Code

- **Added:** ~80 lines (multi-signal detection + documentation)
- **Removed:** ~50 lines (END marker logic)
- **Modified:** ~70 lines (reward values, penalty application, logging)
- **Net change:** +30 lines

### Backward Compatibility

**Breaking changes:**
- Completions will no longer have `(* END *)` marker
- Log files won't have `structural` or `runaway_penalty_applied` fields
- New fields: `base_reward`, `prose_penalty_applied`, `is_degenerate`

**Configuration changes:**
- Can remove `RUNAWAY_PENALTY_MULTIPLIER` from `.envrc` (no longer used)
- New optional: `GRPO_TOP_ENTROPY_QUANTILE` (defaults to 0.2)

---

## References

### Research Papers

1. **SPINE: Token-Selective Test-Time Reinforcement Learning**
   https://arxiv.org/html/2511.17938
   *Finding: Entropy regularization not good fit for GRPO*

2. **GTPO and GRPO-S: Token and Sequence-Level Reward Shaping**
   https://arxiv.org/html/2508.04349v1
   *Advanced entropy weighting methods*

3. **EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction**
   https://arxiv.org/html/2507.21848v1
   *Entropy-based advantage diversity*

4. **Advantage Shaping as Surrogate Reward Maximization**
   https://arxiv.org/html/2510.23049
   *Pass@k training and advantage shaping*

5. **Beyond the 80/20 Rule Paper**
   https://huggingface.co/docs/trl/main/en/grpo_trainer
   *Entropy-based token filtering*

### TRL Documentation

1. **TRL v0.20.0 Release Notes**
   https://github.com/huggingface/trl/releases/tag/v0.20.0
   *Entropy filtering feature*

2. **TRL GRPOConfig Source**
   https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_config.py
   *Available configuration parameters*

3. **RLHF Book - Policy Gradient Algorithms**
   https://rlhfbook.com/c/11-policy-gradients.html
   *EOS token handling in GRPO*

### Critical Analysis

1. **DeepSeek's Lies: A Closer Look at GRPO Implementation**
   https://medium.com/intelligence-factory/deepseeks-lies-a-closer-look-at-grpo-implementation-dea4607842e9
   *Critical analysis of binary reward claims*

---

## Appendices

### Appendix A: Reward Structure Comparison Table

| Error Count | Old Type Score | New Type Score | Absolute Change | Relative Change |
|-------------|----------------|----------------|-----------------|-----------------|
| 0 errors | 0.20 | **0.25** | +0.05 | +25% |
| 1 error | 0.15 | **0.20** | +0.05 | +33% |
| 2 errors | 0.10 | **0.15** | +0.05 | +50% |
| 3 errors | 0.06 | **0.10** | +0.04 | +67% |
| 4 errors | 0.03 | **0.05** | +0.02 | +67% |
| 5+ errors | 0.02 | 0.02 | 0.00 | 0% |

**Total type checking budget:** 20% ‚Üí 25% (+25%)
**Average increase (1-4 errors):** +51%

### Appendix B: Multi-Signal Detection Examples

**Example 1: Pure Code (No Penalty)**
```ocaml
let rec factorial n =
  match n with
  | 0 -> 1
  | n -> n * factorial (n - 1)
```
- Signal 1 (prose): ‚ùå No conversational patterns
- Signal 2 (keywords): ‚ùå High density (let, rec, match, with)
- Signal 3 (repetition): ‚ùå No repetitive chunks
- Signal 4 (purity): ‚ùå 100% code
- **Issues: 0/4 ‚Üí No penalty applied ‚úÖ**

**Example 2: Prose-Wrapped Code (Penalty)**
```
To solve this problem, I'll use recursion. Here's the solution:

let rec factorial n = if n = 0 then 1 else n * factorial (n-1)

This approach is efficient because it uses tail recursion.
```
- Signal 1 (prose): ‚úÖ "To solve", "Here's", "This approach"
- Signal 2 (keywords): ‚ùå High density in code portion
- Signal 3 (repetition): ‚ùå No repetition
- Signal 4 (purity): ‚úÖ Code <50% of completion
- **Issues: 2/4 ‚Üí Penalty applied (√ó0.3) ‚ö†Ô∏è**

**Example 3: Gibberish (Penalty)**
```ocaml
let x = y z w q r s t u v
let a = b c d e f g h i j
foo bar baz qux quux
```
- Signal 1 (prose): ‚ùå No conversational patterns
- Signal 2 (keywords): ‚úÖ Very low density (only "let")
- Signal 3 (repetition): ‚úÖ Repetitive structure
- Signal 4 (purity): ‚ùå All "code" (but nonsense)
- **Issues: 2/4 ‚Üí Penalty applied (√ó0.3) ‚ö†Ô∏è**

**Example 4: BEGIN/END Spam (Penalty)**
```
(* BEGIN *)
(* END *)
(* BEGIN *)
(* END *)
...
```
- Signal 1 (prose): ‚ùå No conversational patterns
- Signal 2 (keywords): ‚úÖ Zero keywords
- Signal 3 (repetition): ‚úÖ Extremely repetitive
- Signal 4 (purity): ‚ùå N/A (no extracted code)
- **Issues: 2/4 ‚Üí Penalty applied (√ó0.3) ‚ö†Ô∏è**

### Appendix C: Expected Value Calculations

**Setup:**
- Success rate: 2% (from previous report)
- Failure rate: 98%

**Strategy A: Try to write correct code**
```
Outcome distribution:
- 98% ‚Üí Fail with various error counts
  - 40% ‚Üí 5+ errors (0.02 reward)
  - 30% ‚Üí 3-4 errors (0.05-0.10 reward)
  - 20% ‚Üí 1-2 errors (0.15-0.20 reward)
  - 8% ‚Üí 0 errors but compile fails (0.25 reward)
- 2% ‚Üí All tests pass (1.00 reward)

Expected value:
= 0.40*0.02 + 0.30*0.075 + 0.20*0.175 + 0.08*0.25 + 0.02*1.00
= 0.008 + 0.0225 + 0.035 + 0.02 + 0.02
= 0.1055 reward per attempt
```

**Strategy B: Generate gibberish avoiding prose**
```
Outcome distribution:
- 100% ‚Üí Gibberish (5+ errors, no prose)
  - Reward: 0.02-0.05

Expected value:
= 1.00 * 0.035
= 0.035 reward per attempt
```

**Strategy C: Generate prose-wrapped gibberish**
```
Outcome distribution:
- 100% ‚Üí Gibberish with prose penalty
  - Base reward: 0.02-0.05
  - Penalty: √ó0.3
  - Final: 0.006-0.015

Expected value:
= 1.00 * 0.01
= 0.01 reward per attempt
```

**Ranking:**
1. Strategy A (real attempts): **0.1055** ‚Üê BEST
2. Strategy B (gibberish, no prose): 0.035
3. Strategy C (gibberish + prose): 0.01

**Conclusion:** New system makes real code attempts 3x more valuable than gibberish!

### Appendix D: Configuration Reference

**Environment Variables:**

| Variable | Default | Purpose |
|----------|---------|---------|
| `TRAINING_DATASET` | `kiranpg/ocaml-training-problems` | Dataset ID |
| `GRPO_NUM_GENERATIONS` | 6 | Samples per problem |
| `GRPO_TEMPERATURE` | 1.0 | Sampling temperature |
| `GRPO_MAX_PROMPT` | 704 | Max prompt tokens |
| `GRPO_MAX_COMPLETION` | 600 | Max completion tokens |
| `GRPO_LEARNING_RATE` | 1e-6 | Learning rate |
| `GRPO_BETA` | 0.05 | KL penalty coefficient |
| `GRPO_TOP_ENTROPY_QUANTILE` | 0.2 | **NEW:** Entropy filtering |
| ~~`RUNAWAY_PENALTY_MULTIPLIER`~~ | ~~0.3~~ | **REMOVED** |

---

**End of Report**
