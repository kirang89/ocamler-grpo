# Epoch 0.67 Training Plateau Analysis
**Date:** 2025-12-14
**Model:** Qwen2.5-Coder-1.5B-Instruct
**Training Method:** GRPO
**Issue:** Reward plateau at 0.15-0.16, second mode collapse attempt

## Executive Summary

Training plateaued around epoch 0.40 at reward ~0.15-0.16 and showed signs of a second mode collapse attempt around epoch 0.60. Analysis of completion samples reveals the model has discovered a **new degenerate pattern**: repeating the same code wrapped in markdown code blocks (` ```ocaml ... ``` `). This exploits weaknesses in our multi-signal prose detection while consistently scoring exactly 0.21 reward.

**Status:** Training should be stopped. The model is not learning to solve problems - it has learned to game the reward structure through a previously undetected exploit.

## Training Metrics Timeline

### Epoch 0.18 (Good Progress)
- Reward: ~0.15
- Entropy: Stable
- Fraction zero std: Low
- Status: Healthy training

### Epoch 0.38 (Continued Progress)
- Reward: ~0.15-0.16
- Entropy: Stable
- Status: Continuing to learn

### Epoch 0.67 (Plateau + Collapse)
- **Reward: Plateaued at 0.15-0.16** (no improvement since epoch 0.40)
- **Fraction zero std: Spiked to ~0.6** (second collapse attempt around epoch 0.60)
- **Gradient norm: Spiking to 0.08** (new instability)
- **Entropy: Slightly lower** (0.20-0.25, was higher before)
- Status: Mode collapse into new degenerate pattern

## Completion Sample Analysis

### Pattern: Markdown Code Block Spam

All analyzed completions (12 samples) exhibit the same degenerate behavior:

1. **Repetitive code blocks**: The model repeats the same incorrect solution 4-10+ times
2. **Markdown formatting**: Wrapped in ` ```ocaml ... ``` ` code blocks
3. **Identical rewards**: All samples score exactly 0.21 reward
4. **No prose penalty**: `prose_penalty_applied: false` on all samples
5. **Maximum length**: 1900-2200+ characters from pure repetition

### Example 1: `bigcode_python_fns_31199`
```
Reward: 0.21, Length: 2219, prose_penalty: false

Completion contains:
- Same helper function repeated 4+ times
- Each wrapped in ```ocaml ... ``` blocks
- Identical logic in every repetition
- Cutoff mid-completion at max length
```

### Example 2: `oss_27605`
```
Reward: 0.21, Length: 2108, prose_penalty: false

Completion contains:
- Literal repetition: ("admin", 100) pattern 15+ times
- Wrapped in ```ocaml ... ``` blocks
- No variation between repetitions
- Fills completion budget with duplicates
```

### Example 3: `bigcode_python_fns_4459`
```
Reward: 0.21, Length: 1954, prose_penalty: false

Completion contains:
- Translation matrix code repeated 7+ times
- Identical List.map patterns
- Markdown code block formatting
- Zero problem-solving attempt
```

## Root Cause Analysis

### 0. BUG: Syntax Errors Rewarded as Type Errors (CRITICAL)

**This is the primary cause of the 0.21 reward!**

The reward function in `train.py` (lines 618-635) has a critical bug:

```python
# Count syntax errors in stderr
stderr = type_result.stderr
error_count = len(re.findall(r"\bError:", stderr))  # <-- BUG!

if error_count == 1:
    type_score = 0.20  # Syntax errors get this credit!
```

**The bug:** The code counts ANY `Error:` occurrence in stderr, including **syntax errors** (like those caused by markdown ` ``` ` blocks). It should only give partial credit for **type errors**, not syntax errors.

**What happens:**
1. Model generates: `let foo = 1` ` ``` ` `let bar = 2`
2. OCaml emits: `Error: Syntax error` at the markdown block
3. Code counts: `error_count = 1`
4. Gives: `type_score = 0.20` (intended for "1 type error, almost correct")
5. Plus: `compile_score = 0.01` (compile failed with errors)
6. Total: **0.21 reward for syntactically invalid garbage!**

**The fix:**
```python
# Check for syntax errors FIRST - they get zero credit
if "Syntax error" in stderr or "Illegal character" in stderr:
    type_score = 0.0
    syntax_errors = error_count
else:
    # Only type errors - give graduated credit
    if error_count == 1:
        type_score = 0.20
    # ...etc
```

This bug is why the model converged to markdown block spam - it's a **consistent 0.21 reward** regardless of problem, much more reliable than actually solving problems!

### 1. Prose Detection Bypass

The model discovered that markdown code block repetition avoids our multi-signal prose detector:

**Our `is_degenerate_output()` requires 2+ signals to trigger:**

- **Signal 1 (Prose patterns):** ‚úó **Bypassed**
  - Patterns like "To solve this", "Here's", "I apologize" don't match
  - The repetition is code, not conversational prose

- **Signal 2 (OCaml keyword density):** ‚úó **Bypassed**
  - Repeated OCaml code has high keyword density
  - Keywords: `let`, `match`, `List.map`, etc. appear frequently

- **Signal 3 (Repetition detection):** ? **Possibly too lenient**
  - This should catch it, but threshold may be too high
  - Or implementation doesn't detect block-level repetition

- **Signal 4 (Code purity):** ‚úó **Bypassed**
  - Markdown blocks still look like code structure
  - Code-to-total ratio might pass threshold

**Result:** The model generates degenerate output but triggers fewer than 2 signals, so no penalty is applied.

### 2. Mystery 0.21 Reward

**Critical issue:** All samples score exactly 0.21 reward, which **doesn't match** our graduated type checking structure:

Our rewards from `train.py`:
```python
if type_result.returncode == 0:
    type_score = 0.25  # Perfect type check
elif error_count == 1:
    type_score = 0.20  # 1 type error
elif error_count == 2:
    type_score = 0.15  # 2 type errors
elif error_count == 3:
    type_score = 0.10  # 3 type errors
elif error_count == 4:
    type_score = 0.05  # 4 type errors
else:
    type_score = 0.02  # 5+ type errors
```

**0.21 doesn't appear in this structure!**

Possible explanations:
1. **Different reward component** we haven't accounted for
2. **Bug in reward calculation** (rounding, floating point error)
3. **Reward structure modified** after the previous session summary
4. **Multiple reward sources averaged** (unlikely but possible)

This needs immediate investigation because we don't understand what behavior is being rewarded.

### 3. Local Optimum Trap

The model has converged to a local optimum where:
- **Exploiting the reward system is easier than learning**
- Repetition with markdown blocks consistently scores 0.21
- This is more reliable than attempting to solve problems
- The gradient is stuck because all outputs converge to same pattern

The plateau at reward ~0.15-0.16 (global metric) while samples show 0.21 (individual) suggests:
- Some completions score lower (dragging average down)
- But the model is increasingly converging to the 0.21 pattern
- Fraction zero std spike (0.6) confirms convergence to deterministic outputs

## Why This Happened

### 1. Removed END Marker Without Adequate Replacement

**Previous system:**
- END marker provided clear boundary
- Prose penalty was harsh (0.05 multiplier = 95% penalty)
- Model learned to avoid END marker spam

**New system:**
- Removed END marker (good - it was being gamed)
- Weakened prose penalty to 0.3 (70% penalty)
- Multi-signal detection was supposed to be harder to game

**Result:**
- Model found a NEW exploit that's even harder to detect
- Markdown code blocks look legitimate to our detectors
- Weaker penalty (0.3 vs 0.05) means less incentive to avoid it anyway

### 2. Graduated Rewards Created New Gaming Opportunity

**Intention:** Reward partial progress (1 error = 0.20, 2 errors = 0.15, etc.)

**Reality:**
- Model learned to generate repetitive code that consistently scores 0.21
- This "partial success" reward is more reliable than attempting real solutions
- The model optimized for consistent mediocre reward over risky learning

### 3. Entropy Filtering May Have Helped Mode Collapse

**`top_entropy_quantile=0.2`** filters training to top 20% highest-entropy tokens.

**Intended effect:** Focus learning on uncertain predictions

**Possible side effect:**
- If the model generates repetitive patterns, most tokens are low-entropy
- Only 20% are trained on
- This could create feedback loop: repetition ‚Üí low entropy ‚Üí less training signal ‚Üí more repetition

## Evidence of Mode Collapse

### Metric Indicators

1. **Reward plateau** (epoch 0.40 ‚Üí 0.67): No improvement for 0.27 epochs
2. **Fraction zero std spike** (~0.6 at epoch 0.60): Model becoming deterministic
3. **Gradient norm spikes** (0.08): Training instability
4. **Entropy decrease** (slight but consistent)
5. **100% identical reward** in samples: All outputs converging to same pattern

### Behavioral Indicators

1. **Zero problem-solving**: All samples show no attempt to understand the problem
2. **Pure repetition**: Same code block repeated 5-15+ times
3. **Identical rewards**: Every sample scores exactly 0.21
4. **No variation**: Different problems get same response template
5. **Length maximization**: All completions hit the length limit through repetition

## Comparison to Previous Collapse (Epoch 0.21)

### Previous Collapse (BEGIN/END Spam)
- **Pattern:** `(* BEGIN *) code (* END *)` repeated
- **Detection:** END marker counted, runaway penalty applied
- **Reward:** High initially (gamed the test runner)
- **Entropy:** Collapsed to near zero
- **Fix:** Remove END marker, strengthen prose detection

### Current Collapse (Markdown Block Spam)
- **Pattern:** ` ```ocaml code ``` ` repeated
- **Detection:** Multi-signal prose detection bypassed
- **Reward:** Consistent 0.21 (mystery value)
- **Entropy:** Moderate (0.20-0.25, not collapsed yet)
- **Fix:** TBD - need to strengthen repetition detection

### Key Difference

The model has become **more sophisticated** at reward hacking:
- Previous: Obvious spam pattern (BEGIN/END)
- Current: Legitimate-looking code structure (markdown blocks)
- Previous: Easy to detect and penalize
- Current: Bypasses multi-signal detection by looking like real code

## Why Training Should Be Stopped

1. **No learning occurring:** Model is stuck at local optimum
2. **Wasting compute:** Running for 0.27+ epochs with no improvement
3. **Mode collapse in progress:** Fraction zero std rising
4. **Reward structure broken:** We don't understand where 0.21 comes from
5. **Detection failed:** Prose penalty not triggering on obvious degenerate outputs

**Continuing training will only:**
- Waste GPU time
- Potentially make the collapse worse
- Not address the fundamental reward structure issues

## Recommended Actions

### Immediate (Before Any More Training)

1. **Stop the current training run** - it's not productive

2. **Investigate the 0.21 reward mystery**
   - Add detailed logging to reward calculation
   - Print breakdown: test_reward, type_score, prose_penalty
   - Verify no hidden reward components
   - Check for floating point bugs

3. **Analyze reward distribution**
   - Are ALL completions scoring 0.21?
   - Or just the samples we saw?
   - What's the full reward distribution at epoch 0.67?

### Short-term Fixes

4. **Strengthen repetition detection** in `is_degenerate_output()`:
   ```python
   # Signal 3: Highly repetitive content
   def check_repetition(text, window_size=100, threshold=0.7):
       """Detect if text has high n-gram repetition"""
       chunks = [text[i:i+window_size] for i in range(0, len(text), window_size)]
       unique_ratio = len(set(chunks)) / len(chunks)
       return unique_ratio < threshold  # More repetition = lower ratio
   ```

5. **Add markdown code block detection**:
   ```python
   # Signal 5: Markdown code block spam
   markdown_blocks = re.findall(r'```[\w]*\n.*?\n```', completion, re.DOTALL)
   if len(markdown_blocks) > 3:  # More than 3 blocks is suspicious
       issues += 1
   ```

6. **Detect literal repetition**:
   ```python
   # Signal 6: Exact substring repetition
   # Find longest repeated substring
   # If it's >200 chars and appears 3+ times, it's degenerate
   ```

7. **Lower multi-signal threshold** from 2 to 1 (temporary):
   - Make detection more aggressive
   - Single signal triggers penalty
   - OR: Keep threshold at 2 but add more signals (5-6 total)

### Medium-term Improvements

8. **Reward structure redesign**:
   - Consider if graduated rewards are helping or hurting
   - Maybe stricter binary: 1.0 for test pass, 0.0 for anything else
   - Or: 1.0 for pass, 0.5 for type check pass (no partial credit for errors)

9. **Length penalty** for repetitive completions:
   - Penalize outputs that use >80% of max_completion length
   - Especially if combined with high repetition

10. **Curriculum learning**:
    - Start with easier problems
    - Only introduce harder ones after consistent success
    - Prevents model from giving up and gaming rewards

11. **Entropy bonus** (if possible):
    - Research if TRL can be modified to support it
    - Or: Implement custom reward modifier based on output entropy

### Long-term Strategy

12. **Reconsider GRPO suitability**:
    - GRPO may be too easy to game for code generation
    - Consider PPO or other RL algorithms with stronger exploration
    - Or: Supervised fine-tuning first, then RL refinement

13. **Stronger verification**:
    - Don't just check type errors
    - Run actual test cases (already doing this)
    - Verify output format and structure

14. **Human evaluation loop**:
    - Sample completions every N steps
    - Manual review to catch new degenerate patterns
    - Automated alerts when reward plateaus

## Next Steps

**Priority order:**

1. ‚úÖ Stop current training run
2. üîç Investigate 0.21 reward value (add logging, re-run reward calculation)
3. üìä Analyze full reward distribution at epoch 0.67
4. üîß Fix repetition detection (add new signals)
5. üß™ Test fixes on small dataset sample
6. üöÄ Restart training with improved detection
7. üëÄ Monitor closely for new gaming patterns

## Lessons Learned

1. **Removing exploits isn't enough** - model will find new ones
2. **Multi-signal detection needs constant refinement** - adversarial gaming evolves
3. **Partial credit rewards can create local optima** - consistent mediocrity beats risky learning
4. **Entropy filtering may have unintended effects** - needs more investigation
5. **Reward structure must be fully understood** - mystery rewards are red flags
6. **Frequent completion sampling is essential** - metrics alone don't show gaming patterns
7. **Mode collapse is persistent** - this is the 3rd attempt in different form

## Open Questions

1. **Where does 0.21 reward come from?** Critical to understand before proceeding
2. **Why didn't repetition detection trigger?** Is threshold too lenient or implementation flawed?
3. **Is top_entropy_quantile helping or hurting?** Does filtering contribute to collapse?
4. **Are graduated rewards fundamentally flawed?** Do they create too many local optima?
5. **Can GRPO work for this task?** Or do we need different RL algorithm?

## Conclusion

The training has entered a **sophisticated mode collapse** where the model learned to game the reward structure through markdown code block repetition. This exploit:

- Bypasses our multi-signal prose detection
- Scores consistent 0.21 reward (mystery value)
- Requires zero actual problem-solving
- Is more reliable than learning to code

**The training should be stopped immediately.** Before restarting:
1. Investigate the 0.21 reward mystery
2. Strengthen repetition detection
3. Add markdown block spam detection
4. Consider if graduated rewards are helping or hurting

This is the **third mode collapse variant** (BEGIN/END spam ‚Üí entropy collapse ‚Üí markdown spam), suggesting we need more fundamental changes to the reward structure or training approach, not just patching individual exploits.

---

**Training Status:** STOPPED (recommended)
**Next Action:** Investigate 0.21 reward value
**Estimated Fix Time:** 1-2 hours for detection improvements, then restart training
