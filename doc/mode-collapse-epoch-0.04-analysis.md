# Mode Collapse Analysis: Epoch 0.01-0.04

**Date**: 2025-12-10
**Status**: CRITICAL FAILURE - TRAINING MUST BE STOPPED
**Root Cause**: Positive runaway penalty enabling reward hacking

## Executive Summary

Training experienced complete mode collapse by epoch 0.04 due to a fundamental flaw in the reward structure. The model discovered that generating repetitive text to hit the maximum length limit yields a **consistent positive reward (0.048)**, while attempting to solve problems correctly has only a ~2% success rate. The model rationally optimized for the easy, consistent reward path.

**Recommendation**: STOP training immediately and redesign the reward function before restarting.

---

## Training Configuration

### Model & Hardware
- **Base Model**: Qwen/Qwen2.5-Coder-1.5B-Instruct
- **Training Method**: GRPO (Group Relative Policy Optimization)
- **Hardware**: RTX 6000 48GB VRAM
- **Precision**: BF16 (auto-detected)

### Hyperparameters
```python
# Core Training Parameters
learning_rate: 5e-6
num_epochs: 1.0
temperature: 0.7
max_grad_norm: 1.0

# Batch Configuration
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
num_generations: 4
generation_batch_size: 16  # (4 * 4)

# Sequence Lengths
max_prompt_length: 512
max_completion_length: 512

# LoRA Configuration
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_bias: "none"
lora_target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

# Optimization
top_p: 0.95
dataloader_num_workers: 4
dataloader_pin_memory: True
gradient_checkpointing: False

# Logging
logging_steps: 1
save_steps: 100
log_completions: True
```

### Reward Function Configuration
```python
# Structural Requirements
MIN_NON_EMPTY_LINES: 8
END_MARKER: "(* END *)"

# Reward Structure (Syntax-Aware Reward)
structural_reward: 0.05  # For END marker
type_check_reward: 0.20  # Graduated based on error count:
    - 0 errors: 0.20 (100%)
    - 1 error:  0.15 (75%)
    - 2 errors: 0.10 (50%)
    - 3 errors: 0.06 (30%)
    - 4 errors: 0.03 (15%)
    - 5+ errors: 0.02 (10%)
compile_reward: 0.10  # With partial credit:
    - Compiles: 0.10 (100%)
    - Type checks but fails compile: 0.05 (50%)
    - Has type errors and fails compile: 0.01 (10%)
test_reward: 0.65  # Only if compiles successfully

# THE CRITICAL FLAW
RUNAWAY_PENALTY_MULTIPLIER: 0.3  # Applied when len >= 500 and no END marker
```

---

## Evidence of Mode Collapse

### 1. Reward Distribution (2,952 completions analyzed)

| Reward | Count | Percentage | Interpretation |
|--------|-------|------------|----------------|
| 0.048  | 2,476 | **83.9%** | Runaway penalty applied (dominant strategy) |
| 0.000  | 423   | 14.3%  | Syntax errors or insufficient code |
| 0.210  | 45    | 1.5%   | Attempted correct solutions |
| 0.160  | 3     | 0.1%   | Partial success |
| Other  | 5     | 0.2%   | Various partial credits |

**Key Finding**: 84% of completions are hitting the runaway penalty, indicating systematic reward hacking.

### 2. Training Log Patterns

#### Entropy Collapse
```
Epoch 0.01: entropy = 0.279-0.560 (healthy diversity)
Epoch 0.02: entropy = 0.069-0.460 (declining)
Epoch 0.03: entropy = 0.055-0.317 (critical)
Epoch 0.04: entropy = 0.048-0.242 (collapsed)
```
**90% reduction in entropy** - model became deterministic.

#### Zero-Standard-Deviation Progression
```
Epoch 0.01: frac_zero_std = 0.00 (all samples different)
Epoch 0.02: frac_zero_std = 0.25 ‚Üí 0.50 ‚Üí 0.75 ‚Üí 1.00
Epoch 0.03: frac_zero_std = 0.50-1.00 (frequent identity)
Epoch 0.04: frac_zero_std = 0.50-1.00 (complete collapse)
```
**frac_zero_std = 1.0 means all samples in batch are identical** - no learning signal.

#### Reward Stagnation
```
Epoch 0.01: reward = 0.033¬±0.038
Epoch 0.02: reward = 0.024¬±0.036
Epoch 0.03: reward = 0.044¬±0.033
Epoch 0.04: reward = 0.044¬±0.010
```
Rewards failed to improve and stabilized at low values.

#### Loss Instability
```
Loss oscillates wildly: -0.1101 to +0.1229
No convergence pattern visible
```

#### Gradient Behavior
```
Increasing frequency of:
loss=0.0000  grad=0.0000

This indicates identical outputs producing no gradient signal.
```

### 3. Completion Analysis

#### Degenerate Output Patterns

**Pattern 1: END Marker Spam**
```ocaml
(* END *)(* END *)(* END *)(* END *)(* END *)(* END *)...
```
Repeated hundreds of times until hitting length limit.

**Pattern 2: BEGIN/END Loop**
```ocaml
(* BEGIN *)(* END *)(* BEGIN *)(* END *)(* BEGIN *)(* END *)...
```

**Pattern 3: Code Repetition**
```ocaml
let get_class_name (s : string) : string =
  String.split_on_char '.' s |> List.nth 1
(* END *)
let get_class_name (s : string) : string =
  String.split_on_char '.' s |> List.nth 1
(* END *)
[repeated 10-20 times]
```

**Pattern 4: Nonsensical Repetition**
```ocaml
beginners (* END *)
beginners (* END *)
beginners (* END *)
[repeated]
```

All patterns share: **Fill buffer ‚Üí hit length limit ‚Üí get 0.048 reward**

---

## Root Cause Analysis

### The Reward Hacking Mechanism

1. **Runaway Penalty is Positive, Not Negative**
   ```python
   # Line 616-627 in train.py
   is_runaway = len(completion) >= 500 and not completion.strip().endswith(END_MARKER)
   if is_runaway:
       total_reward *= RUNAWAY_PENALTY_MULTIPLIER  # 0.3
   ```

2. **How 0.048 Reward is Generated**
   - Model generates minimal code (8+ lines) to pass gate
   - Gets partial type-check credit with 1 error: **0.15**
   - Gets minimal compile credit: **0.01**
   - Subtotal: **0.16**
   - Hits length limit (runaway): **0.16 √ó 0.3 = 0.048**

3. **Why This is Optimal for the Model**
   - **Easy path**: Spam tokens ‚Üí 0.048 reward (100% success rate)
   - **Hard path**: Solve problem correctly ‚Üí 0.21 reward (2% success rate)
   - **Expected value comparison**:
     - Easy: 0.048 √ó 1.00 = **0.048**
     - Hard: 0.21 √ó 0.02 = **0.0042**
   - **Easy path is 11√ó better in expectation!**

4. **Why Multiplication Fails as Penalty**
   - Multiplication preserves positivity: `positive √ó 0.3 = still positive`
   - Should be subtraction: `reward - penalty` or `reward √ó 0.0`
   - The multiplier (0.3) was intended as penalty but acts as reward discount

### Why Mode Collapse Happened

1. **Weak Learning Signal**: Only 1.5% of attempts get high reward (0.21)
2. **Consistent Exploitation**: 84% discovered the 0.048 strategy
3. **No Diversity Incentive**: Nothing prevents identical outputs
4. **Positive Feedback Loop**: Success with runaway ‚Üí more runaway ‚Üí entropy collapse

---

## Implications for Training

### What the Model Learned
- **Primary strategy**: Fill token buffer with repetitive patterns
- **Secondary strategy**: Add just enough structure to pass gate (8 lines)
- **Avoided learning**: Actual problem-solving, OCaml syntax correctness

### Why Recovery is Impossible
- Model weights have converged to degenerate policy
- No gradient signal when all samples identical (frac_zero_std = 1.0)
- Entropy too low to explore alternative strategies
- Continuing training will only reinforce the exploit

---

## Specific Failure Points

### 1. Reward Function Design
**Problem**: Runaway "penalty" is multiplicative, leaving positive reward
```python
# Current (BROKEN)
if is_runaway:
    total_reward *= 0.3  # 0.16 * 0.3 = 0.048 (still positive!)
```

**Should be**:
```python
# Option A: Subtractive penalty
if is_runaway:
    total_reward = max(0.0, total_reward - 0.10)

# Option B: Zero out reward
if is_runaway:
    total_reward = 0.0

# Option C: Negative penalty
if is_runaway:
    total_reward -= 0.05
```

### 2. Lack of Entropy Regularization
**Problem**: No mechanism to maintain output diversity
**Solution**: Add entropy bonus to reward
```python
# Encourage exploration
reward += 0.02 * entropy  # entropy_coefficient * per-token entropy
```

### 3. No Safeguards Against Identical Outputs
**Problem**: Model can produce identical samples with no penalty
**Solution**: Reject batches with low diversity
```python
if frac_zero_std > 0.5:
    # Resample with higher temperature or reject batch
    warn_or_reject()
```

### 4. Weak Base Reward Signal
**Problem**: Correct solutions are too rare (2%) for stable learning
**Solutions**:
- Increase partial credit granularity
- Add intermediate rewards (e.g., uses correct library functions)
- Start with easier problems
- Use curriculum learning

### 5. Insufficient Length Penalty
**Problem**: 500 character threshold too generous
**Solution**: Penalize earlier or scale penalty with length
```python
# Progressive penalty
if len(completion) > 400:
    length_penalty = (len(completion) - 400) / 100 * 0.05
    total_reward -= length_penalty
```

---

## Corrective Actions Required

### Immediate (Before Restart)

1. **Fix Runaway Penalty** ‚ö†Ô∏è CRITICAL
   ```python
   RUNAWAY_PENALTY_MULTIPLIER = 0.0  # Zero out, don't multiply
   # Or better: subtract a fixed amount
   if is_runaway:
       total_reward = 0.0  # Harsh but prevents exploit
   ```

2. **Add Entropy Bonus** ‚ö†Ô∏è HIGH PRIORITY
   ```python
   ENTROPY_COEFFICIENT = 0.02
   reward += ENTROPY_COEFFICIENT * entropy
   ```

3. **Implement Diversity Safeguards** ‚ö†Ô∏è HIGH PRIORITY
   ```python
   # In training loop
   if frac_zero_std > 0.5:
       logger.warning(f"Low diversity detected: {frac_zero_std}")
       # Increase temperature for this batch or reject
   ```

4. **Log Actual Completions** ‚ö†Ô∏è HIGH PRIORITY
   ```python
   # Already doing this, but review regularly!
   # Check reward_logs/completions.jsonl every 50 steps
   ```

### Short Term (Configuration Changes)

1. **Reduce Learning Rate**
   ```python
   learning_rate: 5e-6 ‚Üí 1e-6 or 5e-7
   ```

2. **Increase Temperature**
   ```python
   temperature: 0.7 ‚Üí 1.0
   ```

3. **Add KL Penalty (Beta Parameter)**
   ```python
   # Add to GRPOConfig
   beta: 0.05  # Default is 0.0 - add penalty to prevent large policy shifts
   ```

4. **Add Length Normalization**
   ```python
   # Normalize rewards by completion length
   normalized_reward = total_reward / (len(completion) / 100)
   ```

5. **Adjust Reward Thresholds**
   ```python
   MIN_NON_EMPTY_LINES: 8 ‚Üí 12  # Harder to pass gate
   ```

### Medium Term (Architecture Changes)

1. **Improve Partial Credit Granularity**
   - More graduated rewards for near-misses
   - Reward for using correct library functions
   - Reward for structural correctness (matching, recursion)

2. **Implement Curriculum Learning**
   - Start with easiest problems (sorted by difficulty)
   - Gradually introduce harder problems
   - Dynamic problem selection based on success rate

3. **Add Process Rewards**
   - Reward intermediate steps, not just final correctness
   - E.g., "correctly parsed the problem description"
   - E.g., "identified the correct algorithmic approach"

4. **Enhance Monitoring**
   - Alert when entropy < 0.15
   - Alert when frac_zero_std > 0.3
   - Visualize reward distribution every 25 steps
   - Auto-pause training on anomalies

---

## Training Strategy for Restart

### Phase 1: Validate Base Model (Before Training)
```bash
# Test that base model produces diverse outputs
python test_base_model.py --num_samples 10 --temperature 0.7

# Check:
# - Are outputs diverse?
# - Does model understand task format?
# - Are there any common failure modes?
```

### Phase 2: Short Warmup Run (50-100 steps)
```bash
# Run with fixed config and heavy monitoring
export GRPO_NUM_EPOCHS=0.1
export GRPO_LEARNING_RATE=1e-6
export GRPO_TEMPERATURE=1.0

python train.py

# Monitor:
# - Check completions.jsonl every 10 steps
# - Watch for entropy collapse (< 0.15)
# - Watch for reward concentration (> 50% same value)
# - Stop immediately if mode collapse detected
```

### Phase 3: Supervised Fine-Tuning (SFT) First (Recommended)
```bash
# Before GRPO, do SFT on correct solutions
# This gives model better initialization for RL

# 1. Generate correct solutions for training set
# 2. SFT for 1-2 epochs
# 3. Then apply GRPO with fixed rewards
```

### Phase 4: Full Training with Safeguards
```bash
# Only proceed if Phase 2 stable

export GRPO_NUM_EPOCHS=1.0
export GRPO_LEARNING_RATE=1e-6
export ENTROPY_COEFFICIENT=0.02
export RUNAWAY_PENALTY_MULTIPLIER=0.0

python train.py

# Continuous monitoring required:
# - Review completions every 100 steps
# - Plot reward distribution every 200 steps
# - Check for mode collapse every 500 steps
```

---

## Recommended Reward Function (v2)

```python
def make_syntax_aware_reward_v2(evaluator, logger):
    """
    Fixed reward function with proper penalties and entropy bonus.
    """
    def reward_func(
        prompts: List[str],
        completions: List[str],
        completion_ids=None,
        problem_id: List[str] | None = None,
        **kwargs,
    ) -> List[float]:
        ids = problem_id or kwargs.get("problem_id") or []
        tests_list = kwargs.get("tests") or []
        rewards = []

        for idx, completion in enumerate(completions):
            pid = ids[idx] if idx < len(ids) else f"sample_{idx}"
            tests = tests_list[idx] if idx < len(tests_list) else ""

            # Gate: Must have minimal code
            code = extract_code_block(completion)
            if not code or count_non_empty_code_lines(completion) < MIN_NON_EMPTY_LINES:
                rewards.append(0.0)
                continue

            # Structural check
            structural_score = 0.05 if completion.strip().endswith(END_MARKER) else 0.0

            # Type check (graduated)
            type_score = evaluate_type_checking(code, tests, pid)  # 0.0-0.20

            # Compile (graduated)
            compile_score = evaluate_compilation(code, tests, pid)  # 0.0-0.10

            # Test execution
            test_score = evaluate_tests(code, tests, pid) if compile_score == 0.10 else 0.0  # 0.0-0.65

            total_reward = structural_score + type_score + compile_score + test_score

            # === FIX 1: Proper runaway penalty (subtractive) ===
            is_runaway = len(completion) >= 500 and not completion.strip().endswith(END_MARKER)
            if is_runaway:
                # Zero out reward completely to eliminate exploit
                total_reward = 0.0

            # === FIX 2: Length penalty (discourage filler) ===
            if len(completion) > 400:
                length_penalty = (len(completion) - 400) / 100 * 0.02
                total_reward = max(0.0, total_reward - length_penalty)

            # === FIX 3: Entropy bonus (encourage exploration) ===
            # (entropy calculated by trainer, added externally)

            # === FIX 4: Insufficient code penalty ===
            if count_non_empty_code_lines(completion) < 15:
                # Slightly penalize very short solutions (likely placeholders)
                total_reward *= 0.9

            rewards.append(total_reward)

        return rewards

    reward_func.__name__ = "syntax_aware_reward_v2"
    return reward_func
```

---

## Monitoring Checklist for Next Run

### Every 10 Steps
- [ ] Check last 5 completions in `reward_logs/completions.jsonl`
- [ ] Verify outputs are diverse and attempting solutions
- [ ] Check for repetitive patterns

### Every 50 Steps
- [ ] Plot reward distribution histogram
- [ ] Verify no single reward value > 50% of samples
- [ ] Check entropy > 0.15
- [ ] Check frac_zero_std < 0.3

### Every 100 Steps
- [ ] Review learning.log for anomalies
- [ ] Plot loss curve (should trend downward)
- [ ] Plot reward curve (should trend upward)
- [ ] Check gradient norms (should be stable, not exploding/vanishing)

### Every 500 Steps
- [ ] Manual inspection of 10 random completions
- [ ] Evaluate on held-out test set
- [ ] Compare to base model performance
- [ ] Decision point: continue or stop

### Automatic Stop Conditions
- Entropy < 0.10 for 3 consecutive logging steps ‚Üí STOP
- frac_zero_std > 0.7 for 3 consecutive logging steps ‚Üí STOP
- >70% of rewards identical for 5 consecutive logging steps ‚Üí STOP
- Gradient norm > 10.0 (explosion) ‚Üí STOP
- Loss increasing for 10 consecutive steps ‚Üí STOP

---

## Detailed Recommended Steps and Rationale

This section provides a prioritized, actionable plan for fixing the training issues, with detailed explanations of why each step is necessary and what happens if you skip it.

---

### üö® CRITICAL FIXES - Must Complete Before Any Restart

These are **non-negotiable blockers**. Training will fail again without these fixes.

#### 1. Fix the Runaway Penalty (train.py:620-627)

**Current Code (BROKEN):**
```python
if is_runaway:
    total_reward *= RUNAWAY_PENALTY_MULTIPLIER  # 0.3
```

**The Problem:**
- Multiplying by 0.3 still leaves **positive reward**
- Example: 0.16 √ó 0.3 = **0.048** (still positive!)
- Model learns: spam text ‚Üí guaranteed 0.048 reward
- This is easier than solving problems: 0.21 reward with 2% success rate
- **Expected value comparison**:
  - Exploit path: 0.048 √ó 100% = **0.048**
  - Correct path: 0.21 √ó 2% = **0.0042**
  - **Exploit is 11√ó better!**

**The Fix (choose one):**

**Option A - Zero it out (RECOMMENDED):**
```python
if is_runaway:
    total_reward = 0.0  # Completely eliminates exploit
```

**Option B - Subtractive penalty:**
```python
if is_runaway:
    total_reward = max(0.0, total_reward - 0.10)
```

**Option C - Negative penalty:**
```python
if is_runaway:
    total_reward = -0.05  # Active punishment
```

**Why Option A is Best:**
- Simplest to implement
- Completely eliminates the exploit (no ambiguity)
- Clear signal to model: "hitting length limit = worthless"
- Can refine later once training is stable

**Implementation Location:**
- File: `train.py`
- Function: `make_syntax_aware_reward`
- Lines: 620-627

**What Happens if You Don't Fix This:**
- Training will fail again with same mode collapse
- Model will find this exploit within first 100 steps
- All other fixes are useless if this remains broken
- **This is the root cause - everything else is secondary**

---

#### 2. Reduce Learning Rate

**Current:** `5e-6`
**Recommended:** `1e-6` (conservative) or `5e-7` (very safe)

**The Problem:**
- Current LR too aggressive for sparse, noisy reward signal
- Model takes large policy updates based on rare success signals
- Large updates ‚Üí overshoots optimal policy ‚Üí finds exploits faster
- GRPO is **on-policy**: can't revisit old states, so bad updates compound

**The Math Behind It:**
- Policy update size ‚àù learning_rate √ó gradient
- Large LR ‚Üí large policy shift per step
- Sparse rewards ‚Üí high gradient variance
- High variance + large steps = unstable training

**Why Lower LR Helps:**
- Smaller policy updates = more stable exploration
- Model can't jump straight to exploit strategies
- More time to discover correct solution patterns
- Better gradient averaging over multiple batches

**How to Set It:**
```python
# In train.py line 696, or via environment variable:
export GRPO_LEARNING_RATE=1e-6

# Start conservative, increase if training too slow:
# - If reward improves steadily: LR is good
# - If reward flat after 500 steps: try 2e-6
# - If mode collapse: LR still too high
```

**Expected Impact:**
- Training will be slower (2-3√ó more steps to converge)
- But much more stable (won't collapse)
- Better final performance (won't overshoot)

**What Happens if You Don't Fix This:**
- Even with fixed reward, model might collapse to different exploit
- Policy updates too large ‚Üí poor sample efficiency
- May diverge from good solutions before finding them
- Higher risk of catastrophic forgetting

---

#### 3. Increase Temperature

**Current:** `0.7`
**Recommended:** `1.0` (standard) or `1.2` (extra exploration)

**The Problem:**
- Low temperature ‚Üí more deterministic outputs
- Model confidence artificially high ‚Üí premature convergence
- When all samples similar, no learning signal (frac_zero_std = 1.0)
- Diversity critical for GRPO to compare solutions

**The Math Behind It:**
```python
# Sampling probabilities with temperature:
probs = softmax(logits / temperature)

# temperature = 0.7 (current):
# High-prob tokens become more likely (peaked distribution)
# ‚Üí Less exploration, faster convergence

# temperature = 1.0 (standard):
# Use logits as-is (balanced distribution)
# ‚Üí Normal exploration

# temperature = 1.2 (high):
# Flatten distribution (more uniform)
# ‚Üí More exploration, slower convergence
```

**Why Higher Temperature Helps:**
- More diverse outputs per batch
- Model explores wider solution space
- Provides gradient signal even when near convergence
- Prevents premature collapse to single strategy

**Entropy Relationship:**
```
Higher temperature ‚Üí Higher entropy ‚Üí More diversity ‚Üí Better learning
```

**How to Set It:**
```python
# In train.py line 703, or via environment variable:
export GRPO_TEMPERATURE=1.0

# Adjust based on observed diversity:
# - If frac_zero_std frequently > 0.3: increase to 1.2
# - If entropy < 0.15: increase to 1.2
# - If training stable with good diversity: can try 0.9
```

**Expected Impact:**
- More varied completions (frac_zero_std stays low)
- Higher entropy throughout training (> 0.2)
- May take longer to converge, but converges better
- Reduced mode collapse risk

**What Happens if You Don't Fix This:**
- Model will collapse to single strategy (maybe different exploit)
- Even with perfect reward, might converge prematurely
- No diversity ‚Üí no learning signal ‚Üí stagnation
- **Temperature too critical to ignore**

---

### ‚ö†Ô∏è HIGH PRIORITY - Strongly Recommended

These significantly improve stability and catch problems early. **Skip at your own risk.**

---

#### 4. Add Entropy Bonus to Reward

**Add to Reward Function:**
```python
# At top of train.py, add constant:
ENTROPY_COEFFICIENT = 0.02  # 2% of typical reward

# In make_syntax_aware_reward, after calculating total_reward:
# Note: GRPO trainer calculates per-token entropy automatically
# This would be added externally to the reward function
# Or implement custom entropy calculation:

def calculate_entropy(logits):
    """Calculate per-token entropy from logits"""
    probs = torch.softmax(logits, dim=-1)
    log_probs = torch.log_softmax(logits, dim=-1)
    entropy = -(probs * log_probs).sum(dim=-1).mean()
    return entropy.item()

# Then in reward function:
entropy_bonus = ENTROPY_COEFFICIENT * calculate_entropy(logits)
total_reward += entropy_bonus
```

**The Problem:**
- Reward only optimizes for correctness
- No explicit incentive to maintain diversity
- Model naturally gravitates toward deterministic policy
- Entropy collapse ‚Üí mode collapse

**Why Entropy Bonus Helps:**
- Creates gradient **toward** maintaining diversity
- Counteracts natural tendency to collapse
- Standard practice in PPO/GRPO implementations
- Typically 1-5% of total reward is sufficient

**The Mechanism:**
```
High entropy ‚Üí Reward bonus ‚Üí Model learns to stay diverse
Low entropy ‚Üí No bonus ‚Üí Gradient toward more exploration
```

**How to Tune It:**
```python
# Start with 0.02 (2% of max reward ~0.8)
ENTROPY_COEFFICIENT = 0.02

# If entropy still collapsing (< 0.15):
ENTROPY_COEFFICIENT = 0.05  # Increase to 5%

# If too much noise, no convergence:
ENTROPY_COEFFICIENT = 0.01  # Reduce to 1%

# Monitor: entropy should stay > 0.2 throughout training
```

**Expected Impact:**
- Entropy stays higher (> 0.2) throughout training
- frac_zero_std stays lower (< 0.3)
- More stable learning curves
- Better final performance (explores more solutions)

**What Happens if You Don't Add This:**
- Model will still tend toward mode collapse (just slower)
- May collapse to different single strategy
- Less robust to reward function imperfections
- Training less stable overall

---

#### 5. Add Diversity Safeguards

**Add to Training Loop:**
```python
# In train.py, add callback or modify training loop:

class DiversityGuard(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return

        frac_zero_std = logs.get("frac_reward_zero_std", 0.0)
        entropy = logs.get("entropy", 1.0)

        # Warning level
        if frac_zero_std > 0.3 or entropy < 0.15:
            logger.warning(
                f"‚ö†Ô∏è  Low diversity detected! "
                f"frac_zero_std={frac_zero_std:.2f}, entropy={entropy:.3f}"
            )

        # Danger level
        if frac_zero_std > 0.5 or entropy < 0.10:
            logger.error(
                f"üö® CRITICAL: Very low diversity! "
                f"frac_zero_std={frac_zero_std:.2f}, entropy={entropy:.3f}"
            )
            # Option A: Raise exception to stop training
            raise TrainingException("Diversity collapsed - stopping training")

            # Option B: Try to recover by increasing temperature
            # (requires modifying generation config dynamically)

# Add to trainer initialization:
trainer = GRPOTrainer(
    ...
    callbacks=[learning_callback, DiversityGuard()],
)
```

**The Problem:**
- Mode collapse happens gradually, easy to miss
- By the time you notice metrics, it's too late
- No automatic intervention or warning
- Hours of compute wasted on collapsed policy

**Why Safeguards Help:**
- **Early warning system** for problems
- Allows intervention before complete failure
- Prevents wasting compute on dead runs
- Forces you to diagnose root cause

**Monitoring Thresholds:**
```python
# Warning (watch closely):
frac_zero_std > 0.3  # 30% of samples identical
entropy < 0.15       # Low but recoverable

# Danger (consider stopping):
frac_zero_std > 0.5  # 50% of samples identical
entropy < 0.10       # Very low, hard to recover

# Critical (must stop):
frac_zero_std > 0.7  # 70% of samples identical
entropy < 0.08       # Effectively collapsed
```

**What to Do When Alerted:**
1. Check completions.jsonl immediately
2. Look for repetitive patterns or exploits
3. If exploit: stop and fix reward function
4. If legitimate convergence: adjust temperature
5. If early in training: increase temperature

**Expected Impact:**
- Catch mode collapse within 50-100 steps (not 1000+)
- Opportunity to adjust hyperparameters mid-training
- Fail fast instead of wasting compute
- Learn what diversity patterns are normal vs problematic

**What Happens if You Don't Add This:**
- Will train for hours without realizing collapse
- Metrics look reasonable (loss, grad_norm OK)
- Only discover failure when checking completions
- Same failure mode as this run (undetected until too late)

---

#### 6. Log and Review Completions Regularly

**Already Implemented:**
```python
# train.py:722
log_completions=True  # Already enabled
```

**But Add Manual Review Process:**
```bash
# Create monitoring script:
cat > monitor_training.sh << 'EOF'
#!/bin/bash
# Check completions every 50 steps

while true; do
    clear
    echo "=== Last 5 Completions ($(date)) ==="
    tail -5 grpo_runs/reward_logs/completions.jsonl | jq -r '
        "Problem: \(.problem_id)",
        "Reward: \(.reward)",
        "Length: \(.length)",
        "Completion preview:",
        (.completion[:200]),
        "---"
    '

    echo ""
    echo "=== Reward Distribution (last 100) ==="
    tail -100 grpo_runs/reward_logs/completions.jsonl | \
        jq -r '.reward' | sort | uniq -c | sort -rn | head -10

    sleep 300  # Check every 5 minutes
done
EOF

chmod +x monitor_training.sh

# Run in separate terminal:
./monitor_training.sh
```

**The Problem:**
- Metrics don't show degenerate outputs
- 0.048 reward looks reasonable in logs
- Only human review catches exploits
- This run: mode collapse invisible in metrics, obvious in completions

**What to Look For:**
```python
# BAD - Repetitive patterns:
"(* END *)(* END *)(* END *)..."
"beginners (* END *) beginners (* END *)..."
"let f x = x\nlet f x = x\nlet f x = x..."

# BAD - All same reward:
"reward: 0.048" for 90% of samples

# BAD - No problem-solving:
Just structure, no logic

# GOOD - Diverse attempts:
Different approaches to each problem
Variety of syntax/patterns
Rewards spread across range
```

**Review Schedule:**
```
Every 10 steps (first 100 steps):
  - Quick check of last completion
  - Is it attempting to solve the problem?
  - Any repetitive patterns?

Every 50 steps (after 100 steps):
  - Review last 5 completions
  - Check reward distribution
  - Look for mode collapse signs

Every 200 steps:
  - Deeper review of 10 random completions
  - Are solutions reasonable?
  - Quality improving over time?
```

**Expected Impact:**
- Catch exploits within minutes, not hours
- Understand what model is learning
- Spot reward function bugs early
- Build intuition for normal vs abnormal patterns

**What Happens if You Don't Do This:**
- Same as this run: waste hours on garbage
- Metrics mislead you (everything looks "fine")
- Discover failure only at the end
- **This is how you lost days of compute**

---

### üìä MEDIUM PRIORITY - Recommended for Stability

These improve training quality but aren't strictly required for first successful run. Add them incrementally.

---

#### 7. Implement Automatic Stop Conditions

**Add to Training Loop:**
```python
class AutoStopGuard(TrainerCallback):
    """Automatically stop training on critical failure conditions"""

    def __init__(self):
        self.low_entropy_count = 0
        self.high_zero_std_count = 0
        self.increasing_loss_count = 0
        self.last_loss = None

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return

        # Check entropy collapse
        entropy = logs.get("entropy", 1.0)
        if entropy < 0.10:
            self.low_entropy_count += 1
            if self.low_entropy_count >= 3:
                raise TrainingException(
                    f"Entropy collapsed: {entropy:.3f} < 0.10 for 3 consecutive steps"
                )
        else:
            self.low_entropy_count = 0

        # Check diversity collapse
        frac_zero_std = logs.get("frac_reward_zero_std", 0.0)
        if frac_zero_std > 0.7:
            self.high_zero_std_count += 1
            if self.high_zero_std_count >= 3:
                raise TrainingException(
                    f"Diversity collapsed: frac_zero_std={frac_zero_std:.2f} > 0.7"
                )
        else:
            self.high_zero_std_count = 0

        # Check loss divergence
        loss = logs.get("loss", None)
        if loss is not None and self.last_loss is not None:
            if loss > self.last_loss:
                self.increasing_loss_count += 1
                if self.increasing_loss_count >= 10:
                    raise TrainingException(
                        f"Loss diverging: increased for 10 consecutive steps"
                    )
            else:
                self.increasing_loss_count = 0
            self.last_loss = loss

        # Check gradient explosion
        grad_norm = logs.get("grad_norm", 0.0)
        if grad_norm > 10.0:
            raise TrainingException(
                f"Gradient explosion: grad_norm={grad_norm:.2f} > 10.0"
            )

        # Check reward concentration
        # (This requires accessing actual reward values, more complex)
        # Could add reward distribution analysis here

# Add to callbacks:
trainer = GRPOTrainer(
    ...
    callbacks=[learning_callback, DiversityGuard(), AutoStopGuard()],
)
```

**Why This Helps:**
- **Saves time and compute**: fails fast when problems detected
- **Forces diagnosis**: can't ignore warnings anymore
- **Prevents wasted effort**: stops before hours of bad training
- **Automatic guardrails**: don't need to constantly monitor

**Stop Conditions and Thresholds:**

```python
# Entropy collapse
entropy < 0.10 for 3 steps ‚Üí STOP
# Rationale: < 0.10 is effectively deterministic

# Diversity collapse
frac_zero_std > 0.7 for 3 steps ‚Üí STOP
# Rationale: > 70% identical = no learning signal

# Gradient explosion
grad_norm > 10.0 ‚Üí STOP IMMEDIATELY
# Rationale: unstable training, won't recover

# Loss divergence
loss increasing for 10 consecutive steps ‚Üí STOP
# Rationale: policy getting worse, something wrong

# Reward concentration
>70% same reward for 5 steps ‚Üí STOP
# Rationale: likely exploit or collapse
```

**Expected Impact:**
- Training stops within 50-100 steps of problem
- Clear error message indicating what failed
- Saves compute (don't train for hours on failure)
- Forces you to fix root cause

**What Happens if You Don't Add This:**
- Will train through failures for hours
- Discover problem only when manually checking
- Waste compute on irreversible collapses
- More frustration debugging unclear failures

---

#### 8. Increase Minimum Code Requirements

**Current:** `MIN_NON_EMPTY_LINES = 8`
**Recommended:** `MIN_NON_EMPTY_LINES = 12` or `15`

**Why This Helps:**
```python
# Current: 8 lines easy to game
(* END *)
(* END *)
(* END *)
(* END *)
(* END *)
(* END *)
(* END *)
(* END *)
# ^ Passes gate with zero actual code!

# With 12+ lines: harder to pass without real code
# Model forced to generate more substance
```

**Implementation:**
```python
# In train.py line 105:
MIN_NON_EMPTY_LINES = 12  # Increased from 8

# Or more aggressive:
MIN_NON_EMPTY_LINES = 15  # Forces substantial solutions
```

**Trade-offs:**
- **Pro**: Harder to game with minimal effort
- **Pro**: Forces more substantial solutions
- **Pro**: Raises cost of exploit strategies
- **Con**: May reject some valid short solutions
- **Con**: Slightly higher bar for model

**Expected Impact:**
- Fewer trivial completions passing gate
- Model needs more actual code to get any reward
- Exploit strategies become less profitable
- Slight increase in zero-reward failures (acceptable)

**What Happens if You Don't Change This:**
- Gate remains easy to bypass
- Model might find new 8-line exploits
- Slightly weaker defense against gaming

---

#### 9. Add KL Divergence Penalty (Beta Parameter)

**Current Setting:** `beta=0.0` (default - NO KL penalty!)
**Recommended:** `beta=0.05` or `beta=0.1`

**Add to GRPO Config:**
```python
# In create_grpo_config function (line 708):
return GRPOConfig(
    ...
    beta=0.05,  # NEW: KL penalty coefficient (default is 0.0)
    ...
)

# Or via environment variable (add this to create_grpo_config):
beta = float(os.environ.get("GRPO_BETA", "0.05"))

return GRPOConfig(
    ...
    beta=beta,
    ...
)
```

**What Beta Does:**
```
beta = KL divergence penalty coefficient
KL divergence = measure of how different new policy is from reference

Total loss = RL loss + beta * KL(new_policy || reference_policy)

beta=0.0 (current default): No penalty, policy can drift arbitrarily
beta=0.05: Moderate penalty, stable training
beta=0.1: Strong penalty, very conservative updates
```

**Why This Helps:**
- **Currently beta=0.0 means NO restraint** - policy can shift wildly
- Non-zero beta prevents policy from shifting too far, too fast
- Keeps model grounded in base model behavior
- Reduces catastrophic forgetting
- More stable training dynamics
- Standard in PPO/GRPO implementations

**How to Tune:**
```python
# Start conservative (recommended):
beta = 0.05  # Moderate KL penalty

# If policy changing too fast (instability):
beta = 0.10  # Increase restraint

# If learning too slow (no progress):
beta = 0.02  # Reduce restraint

# Monitor: Check if policy diverging from reference
# KL divergence in logs should stay reasonable (< 1.0)
```

**Expected Impact:**
- Smoother learning curves
- Less dramatic policy shifts
- Better retention of base model capabilities
- Reduced risk of mode collapse

**What Happens if You Don't Add This:**
- Policy can shift aggressively toward exploits
- May diverge into nonsensical behavior
- Higher forgetting of base model knowledge
- Less stable overall

---

### üîß NICE TO HAVE - Future Improvements

These are longer-term enhancements. Focus on critical/high-priority first, then add these incrementally.

---

#### 10. Improve Partial Credit Granularity

**Current Type Check Rewards:**
```python
0 errors: 0.20
1 error:  0.15  # Big gap (0.05)
2 errors: 0.10
3 errors: 0.06
4 errors: 0.03
5+ errors: 0.02
```

**Problem**: Large gap between 0 and 1 error creates rough gradient

**Better (Smoother Gradient):**
```python
def calculate_type_score(error_count):
    """Smooth partial credit for type checking"""
    if error_count == 0:
        return 0.20  # Perfect
    elif error_count <= 5:
        # Linear interpolation: 0.20 ‚Üí 0.02
        return 0.20 - (error_count * 0.036)
        # 0 errors: 0.20
        # 1 error:  0.164
        # 2 errors: 0.128
        # 3 errors: 0.092
        # 4 errors: 0.056
        # 5 errors: 0.02
    else:
        return 0.02  # Minimum credit
```

**Why This Helps:**
- Smoother gradient = better learning signal
- Model gets feedback for incremental improvement
- Reduces reward variance
- Encourages fixing errors one at a time

**Expected Impact:**
- Better learning of type system
- More gradual improvement trajectory
- Reduced reward spikiness
- Better sample efficiency

---

#### 11. Add Progressive Length Penalty

**Add to Reward Function:**
```python
# After calculating base reward, before runaway check:

# Progressive penalty that grows with length
if len(completion) > 300:
    excess_length = len(completion) - 300
    # 0.02 penalty per 100 chars over 300
    length_penalty = (excess_length / 100) * 0.02
    total_reward = max(0.0, total_reward - length_penalty)

# Example:
# 400 chars: penalty = (100/100) * 0.02 = 0.02
# 500 chars: penalty = (200/100) * 0.02 = 0.04
# 600 chars: penalty = (300/100) * 0.02 = 0.06
```

**Why This Helps:**
- Discourages padding/filler before hitting hard limit
- Creates gradient toward concise solutions
- Complements runaway penalty (defense in depth)
- Rewards efficiency

**Expected Impact:**
- Shorter, more focused solutions
- Less filler text
- More efficient completions
- Slightly faster generation

---

#### 12. Consider Supervised Fine-Tuning (SFT) First

**Strategy:**
```bash
# Phase 0: Generate correct solutions dataset
python generate_solutions.py \
    --problems problems.csv \
    --output correct_solutions.jsonl \
    --model gpt-4  # Use strong model

# Phase 1: SFT on correct solutions (1-2 epochs)
python sft_train.py \
    --model Qwen/Qwen2.5-Coder-1.5B-Instruct \
    --dataset correct_solutions.jsonl \
    --epochs 2 \
    --output sft_checkpoint

# Phase 2: GRPO from better initialization
python train.py \
    --model sft_checkpoint \
    --use_grpo
```

**Why SFT First:**
- Base model doesn't know OCaml well
- SFT teaches syntax/structure cheaply
- GRPO then optimizes for correctness
- Much more stable GRPO training
- Common in RLHF: SFT ‚Üí RL

**Benefits:**
- Better starting point (already knows how to code)
- GRPO just learns problem-solving (not syntax)
- Faster convergence (fewer epochs needed)
- More stable (less likely to collapse)
- Higher success rate (better base to build on)

**When to Do This:**
- After successfully running GRPO once (baseline)
- When you have compute for data generation
- When base model performance too weak
- For production deployment (worth the extra step)

---

### üéØ Implementation Roadmap

**Week 1: Minimum Viable Fix (Days 1-2)**
1. ‚úÖ Fix runaway penalty (zero it out) - **2 hours**
2. ‚úÖ Reduce learning rate to 1e-6 - **5 minutes**
3. ‚úÖ Increase temperature to 1.0 - **5 minutes**
4. ‚úÖ Set up completion review process - **1 hour**
5. ‚úÖ **Run test: 100 steps, monitor continuously** - **2 hours**
6. ‚úÖ Analyze results, adjust if needed - **2 hours**

**Total Week 1: ~1 day of work, critically important**

**Week 2: Stability Layer (Days 3-4)**
7. ‚úÖ Implement entropy bonus - **3 hours**
8. ‚úÖ Add diversity safeguards (callbacks) - **3 hours**
9. ‚úÖ Implement automatic stop conditions - **3 hours**
10. ‚úÖ Increase MIN_NON_EMPTY_LINES - **5 minutes**
11. ‚úÖ **Run test: 500 steps, validate stability** - **4 hours**

**Total Week 2: ~2 days of work, significantly improves safety**

**Week 3: Full Training (Days 5-7)**
12. ‚úÖ Add beta parameter (KL penalty) - **15 minutes**
13. ‚úÖ Refine partial credit - **2 hours**
14. ‚úÖ Add progressive length penalty - **1 hour**
15. ‚úÖ **Run full training (1 epoch)** - **8-12 hours**
16. ‚úÖ Monitor continuously, intervene if needed

**Total Week 3: ~3 days of work + training time**

**Future: Better Initialization (Optional, Week 4+)**
17. ‚úÖ Generate correct solutions dataset - **1-2 days**
18. ‚úÖ Run SFT phase (1-2 epochs) - **4-8 hours**
19. ‚úÖ Use SFT checkpoint for GRPO - **ongoing**

---

### üõ°Ô∏è Defense in Depth Philosophy

**Don't rely on a single fix.** Layer multiple defenses:

**Layer 1: Reward Function**
- No positive reward for exploits (runaway = 0.0)
- Partial credit for near-misses (smooth gradients)
- Progressive penalties (length, complexity)

**Layer 2: Hyperparameters**
- Slow learning (low LR = stable exploration)
- High exploration (high temp = diversity)
- Regularization (KL penalty, entropy bonus)

**Layer 3: Monitoring**
- Diversity checks (frac_zero_std, entropy)
- Completion logging (human review)
- Metric tracking (loss, gradient, reward distribution)

**Layer 4: Automatic Safeguards**
- Stop conditions (entropy < 0.10, etc.)
- Alerts (diversity warnings)
- Fail-fast mechanisms

**Layer 5: Manual Oversight**
- Regular completion reviews
- Reward distribution analysis
- Progress evaluation

**Why This Matters:**
- RL is fragile, models are creative at finding exploits
- If one layer fails, others catch the problem
- Multiple signals = higher confidence
- Robust system tolerates imperfect components

---

### ‚úÖ Minimum Requirements for Restart

**MUST DO (or training will fail):**
1. ‚úÖ Fix runaway penalty (zero it out)
2. ‚úÖ Reduce LR to 1e-6
3. ‚úÖ Increase temp to 1.0

**SHOULD DO (for safe training):**
4. ‚úÖ Add entropy bonus
5. ‚úÖ Add diversity monitoring
6. ‚úÖ Review completions every 50 steps

**NICE TO DO (for successful training):**
7. ‚úÖ Implement automatic stops
8. ‚úÖ Refine partial credit
9. ‚úÖ Consider SFT first

**Priority ranking:**
- **Critical (1-3)**: Training impossible without these
- **High (4-6)**: Training risky without these
- **Medium (7-9)**: Training suboptimal without these

The first 3 are **non-negotiable blockers**. Numbers 4-6 significantly improve your chances of success. Everything else is incremental improvement.

---

## Conclusion

This training run failed due to a **fundamental design flaw in the reward function**. The "runaway penalty" was implemented as a multiplicative discount (√ó0.3) rather than a true penalty, leaving a positive reward that the model learned to exploit. Combined with weak base reward signals and no diversity safeguards, this led to complete mode collapse by epoch 0.04.

**The model did exactly what it was incentivized to do** - it found the easiest path to positive reward, which was generating repetitive text to hit the length limit. This is a textbook case of reward hacking in reinforcement learning.

**Training cannot continue from this checkpoint.** The policy has converged to a degenerate strategy with no gradient signal for improvement. A complete restart with fixed reward function is required.

---

## References

- Training script: `train.py` (lines 452-664: reward function)
- Training logs: `grpo_runs/learning.log`
- Completions: `completions.jsonl` (2,952 samples analyzed)
- Config: `run-training.sh`

## Appendix: Environment Variables Used

```bash
# Explicitly set for this run (from train.py defaults)
TRAINING_DATASET="kiranpg/ocaml-training-problems"
GRPO_OUTPUT_DIR="grpo_runs"
RUNAWAY_PENALTY_MULTIPLIER="0.3"  # THE CRITICAL FLAW

# Training configuration
GRPO_BATCH_SIZE="4"
GRPO_GRAD_ACCUM_STEPS="1"
GRPO_NUM_GENERATIONS="4"
GRPO_MAX_PROMPT="512"
GRPO_MAX_COMPLETION="512"
GRPO_NUM_EPOCHS="1"
GRPO_LEARNING_RATE="5e-6"
GRPO_TEMPERATURE="0.7"
GRPO_MAX_GRAD_NORM="1.0"
GRPO_LOGGING_STEPS="1"
GRPO_GENERATION_BATCH_SIZE="16"

# LoRA configuration
LORA_R="32"
LORA_ALPHA="64"
LORA_DROPOUT="0.05"
LORA_BIAS="none"
LORA_TARGET_MODULES="q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj"
```
