export BASE_MODEL_ID=Qwen/Qwen2.5-Coder-1.5B-Instruct

export GRPO_NUM_GENERATIONS=12
export GRPO_TEMPERATURE=1.0
export GRPO_MAX_PROMPT=800
export GRPO_MAX_COMPLETION=700
export GRPO_LEARNING_RATE=5e-6
export GRPO_BETA=0.01
export GRPO_TOP_ENTROPY_QUANTILE=1.0
export GRPO_DISABLE_PROSE_PENALTY=true

export TOKENIZERS_PARALLELISM=true
export REWARD_POOL_SIZE=8

# Inference speed optimizations (Linux + CUDA only)
export GRPO_USE_FLASH_ATTN=true        # Flash Attention 2 (requires Ampere+ GPU)
export GRPO_USE_TORCH_COMPILE=false    # torch.compile (slower first step, then faster)

export PYTHONUNBUFFERED=1
export TRAINING_DATASET=kiranpg/ocaml-training-problems
