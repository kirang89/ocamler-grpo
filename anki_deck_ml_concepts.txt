# Reinforcement Learning & Training Concepts Anki Deck
# Import Instructions: In Anki, go to File > Import, select this file, choose "Basic" or "Cloze" type based on card

## BASIC CARDS (Front	Back format)
What is a policy in reinforcement learning?	A policy is a mapping from states to actions that defines how an agent behaves. In language models, it's the probability distribution over possible next tokens given the current context.
What is the difference between on-policy and off-policy RL?	On-policy methods learn about the policy they're currently executing (e.g., SARSA, GRPO). Off-policy methods learn about a different policy than the one generating data (e.g., Q-learning). On-policy is more stable but less sample-efficient.
What is an advantage function in RL?	The advantage function A(s,a) measures how much better an action is compared to the average action in that state. Formally: A(s,a) = Q(s,a) - V(s), where Q is action-value and V is state-value. Positive advantage means better than average.
Why use advantage instead of raw rewards in policy gradient methods?	Using advantages reduces variance in gradient estimates. It tells the model "this action was X better/worse than average" rather than "this action got reward R" (which could be high even if the action was bad). This leads to more stable training.
What is the key innovation of GRPO (Group Relative Policy Optimization)?	GRPO eliminates the need for a value function/critic by computing advantages using relative comparisons within a group of solutions to the same problem. It compares multiple completions for each prompt and uses group statistics to estimate advantages.
Compare GRPO vs PPO: Which requires a critic network?	PPO requires a critic network (value function) to estimate baselines for advantage calculation. GRPO does not - it uses relative comparisons within groups, making it simpler and more stable for some tasks.
What is mode collapse in neural network training?	Mode collapse occurs when a model converges to generating repetitive or degenerate outputs instead of diverse solutions. The model "collapses" to a narrow region of the output space, often exploiting a loophole in the reward function.
What are three warning signs of mode collapse?	1) Low entropy (< 0.10) - model outputs become deterministic<br>2) High frac_zero_std - most samples get identical rewards<br>3) Reward plateaus while diversity drops - model found a "hack"
Why is entropy important in RL training?	Entropy measures the randomness/diversity of the policy's outputs. High entropy = exploring many options. Low entropy = exploitation/convergence. Without enough entropy, the model may collapse to repetitive outputs or fail to explore better solutions.
What is entropy regularization and why use it?	Adding an entropy bonus to the reward encourages the model to maintain diverse outputs during training. It prevents premature convergence to a single solution and helps exploration. However, too much can destabilize training.
What is KL divergence and why is it used in policy optimization?	KL divergence measures how different two probability distributions are. In RL, it's used to penalize the updated policy from diverging too far from the reference policy, preventing destructive updates that could collapse performance.
What does the "beta" parameter control in PPO/GRPO?	Beta is the coefficient for the KL divergence penalty. Higher beta = stronger constraint keeping the new policy close to the old one. It's a stability vs speed tradeoff: high beta = stable but slow learning, low beta = fast but risky.
What is the difference between sparse and dense rewards?	Sparse rewards: feedback only at final outcome (e.g., 1 if test passes, 0 otherwise). Dense rewards: feedback at every step (e.g., partial credit for each correct intermediate step). Dense rewards are easier to learn from but harder to design.
What is reward shaping?	Reward shaping modifies the reward function to provide more informative feedback while preserving the optimal policy. Example: instead of 0/1 for fail/pass, use graduated rewards like 0.25 for type-checking, 0.60 for compilation, 1.0 for passing tests.
What is reward hacking and how does it happen?	Reward hacking is when a model finds unintended ways to maximize reward without solving the actual task. Example: generating repetitive tokens that happen to pass a simple regex check, or exploiting edge cases in the reward function.
How can you prevent reward hacking?	1) Use multi-signal detection (require multiple independent checks)<br>2) Design robust reward functions (graduated, not binary)<br>3) Monitor for degenerate outputs<br>4) Use verifiable feedback when possible<br>5) Regular human evaluation
What is the gradient in gradient descent?	The gradient is the vector of partial derivatives of the loss with respect to each parameter. It points in the direction of steepest increase of the loss. We move in the opposite direction (negative gradient) to minimize loss.
What does learning rate control?	Learning rate controls the step size when updating parameters. Too high = unstable training, overshooting minima. Too low = slow training, may get stuck. It's the most important hyperparameter to tune.
What is gradient explosion and how to detect it?	Gradient explosion is when gradients become extremely large, causing parameter updates to overshoot and destabilize training. Detected by: grad_norm > 10, loss becoming NaN, or parameters diverging. Solution: gradient clipping or lower learning rate.
What is gradient vanishing and how to detect it?	Gradient vanishing is when gradients become extremely small, causing learning to stop. Detected by: grad_norm near 0, loss not decreasing, parameters not updating. Common in deep networks. Solutions: better initialization, normalization, or architecture changes.
What is gradient clipping?	Gradient clipping limits the magnitude of gradients to prevent explosion. If gradient norm exceeds a threshold, scale it down proportionally. This stabilizes training without changing the gradient direction.
What is a loss function?	A loss function (or objective function) quantifies how badly the model is performing. It's a single number we want to minimize. For RL: often negative expected reward. For supervised learning: difference between predictions and targets.
Why might loss be negative in GRPO?	In policy gradient methods, loss = -advantage, meaning we're maximizing advantages. Negative loss means the model is being pushed toward actions with positive advantages (better than average). More negative = stronger push toward good actions.
What does "overfitting" mean?	Overfitting is when a model learns the training data too well, including noise and specifics, failing to generalize to new data. Signs: training loss keeps decreasing while validation loss increases.
How can you detect overfitting?	1) Monitor train vs validation metrics - divergence indicates overfitting<br>2) Model performs well on training data but poorly on test data<br>3) Validation loss starts increasing while training loss decreases
What is the train/validation/test split for?	Training set: used to update model parameters. Validation set: used to tune hyperparameters and detect overfitting. Test set: final evaluation of model performance, never seen during development. Prevents "overfitting to the validation set."
What is LoRA (Low-Rank Adaptation)?	LoRA is a parameter-efficient fine-tuning method that freezes the original model weights and adds small trainable "adapter" matrices. Instead of updating millions of parameters, only update thousands in low-rank matrices that get added to the original weights.
Why use LoRA instead of full fine-tuning?	1) Memory efficient - only store adapter weights<br>2) Faster training - fewer parameters to update<br>3) Easier to distribute - share small adapter files instead of full model<br>4) Less prone to catastrophic forgetting<br>5) Can train multiple adapters for different tasks
What does "rank" mean in LoRA?	The rank determines the size of the adapter matrices. Higher rank = more parameters = more expressive but more memory. Typical values: 8-64. Low rank exploits the idea that weight updates lie in a low-dimensional subspace.
What is Supervised Fine-Tuning (SFT)?	SFT is training a pre-trained model on labeled input-output pairs using supervised learning (cross-entropy loss). In LLM fine-tuning, it's often used as initialization before reinforcement learning to teach basic task structure.
Why do SFT before reinforcement learning?	SFT provides a good initialization by teaching the model basic task structure and syntax. Starting RL from a random or base model is inefficient and unstable. SFT → RL is the standard pipeline for RLHF/RLVR.
What is RLVF (Reinforcement Learning from Verifiable Feedback)?	RLVF uses automated verification systems (compilers, test suites, formal provers) instead of human feedback to generate rewards. It's a subset of RLHF that's infinitely scalable since verification is automated.
Compare RLHF vs RLVF	RLHF (Human Feedback): humans provide rewards, expensive and slow to scale, subjective. RLVF (Verifiable Feedback): automated systems (compilers, tests) provide rewards, free and scalable, objective. RLVF only works when verification is possible.
What is an epoch in training?	One epoch is a complete pass through the entire training dataset. If you have 1000 examples and process them all once, that's 1 epoch. Training typically runs for multiple epochs (10, 100, etc.).
What is batch size?	Batch size is the number of training examples processed together before updating parameters. Larger batches = more stable gradients but more memory. Smaller batches = noisier gradients but better generalization. Typical: 8-128.
What is gradient accumulation?	Gradient accumulation simulates larger batch sizes by accumulating gradients over multiple small batches before updating parameters. Useful when GPU memory limits batch size. Accumulating over 4 batches = 4x effective batch size.
What does convergence mean in training?	Convergence means the model has reached a stable point where further training doesn't significantly improve the loss. The loss curve flattens out. Note: converging doesn't always mean good performance - you might converge to a bad solution!
What is a learning rate schedule?	A learning rate schedule changes the learning rate during training. Common: start high for fast learning, then decay to fine-tune. Example: linear decay, cosine annealing, step decay. Prevents overshooting near convergence.
What is the exploration-exploitation tradeoff?	Exploration: trying new actions to discover better solutions. Exploitation: using known good actions to maximize reward. Too much exploration = never optimize what works. Too much exploitation = miss better solutions. Must balance both.
What is a value function in RL?	A value function V(s) estimates the expected cumulative reward from a state following the current policy. It answers "how good is this state?" Used in many RL algorithms to guide learning, but not needed in GRPO.
What is a Q-function (action-value function)?	Q(s,a) estimates the expected cumulative reward from taking action a in state s, then following the policy. It answers "how good is this action in this state?" Used in Q-learning, actor-critic methods.
What is temporal difference (TD) learning?	TD learning updates value estimates based on differences between predicted and actual rewards plus next-state value. It learns from incomplete episodes by bootstrapping from current estimates. Example: V(s) ← V(s) + α[r + γV(s') - V(s)]
What is the discount factor (gamma) in RL?	Gamma (γ) determines how much the agent values future rewards vs immediate rewards. γ=0: only care about immediate reward. γ=1: all future rewards equally important. Typical: 0.9-0.99. Balances short-term and long-term planning.
What is on-policy vs off-policy learning?	On-policy: learn about the policy you're currently using to collect data (e.g., SARSA, GRPO). Off-policy: learn about a target policy while collecting data with a different behavior policy (e.g., Q-learning, DQN). Off-policy is more sample-efficient but less stable.
What is the curse of dimensionality in RL?	As state/action spaces grow, the number of possible states grows exponentially, making it impossible to visit or learn about all states. This is why function approximation (neural networks) is needed for complex tasks.
What is reward_std and why monitor it?	Reward_std (reward standard deviation) measures the diversity of rewards across a batch. High std = model generates solutions with varied performance. Low std = all solutions getting similar rewards, possible mode collapse warning.
What does frac_reward_zero_std mean?	Fraction of problems where all generated solutions received identical rewards (std=0). High values (>0.7) are critical - it means the model is generating identical outputs for different prompts, strong sign of mode collapse.
What is catastrophic forgetting?	Catastrophic forgetting is when a neural network forgets previously learned tasks when trained on new tasks. The new task's gradients overwrite weights important for old tasks. Common in continual learning and fine-tuning.
How can you prevent catastrophic forgetting in fine-tuning?	1) Use parameter-efficient methods like LoRA (freeze base model)<br>2) Mix in data from previous tasks (rehearsal)<br>3) Use regularization to keep weights close to original (KL penalty)<br>4) Lower learning rates for fine-tuning
What is completion-only training?	Training where the loss is only computed on the completion/output tokens, not the prompt/input tokens. The model only learns to generate good outputs, not to memorize prompts. Common in instruction fine-tuning.
Why mask prompt tokens from loss in supervised fine-tuning?	If prompts are included in loss, the model wastes capacity memorizing prompts instead of learning to generate good completions. Masking focuses learning on the actual task: generating appropriate outputs.
What is a baseline in policy gradient methods?	A baseline is a reference value subtracted from rewards to reduce variance in gradient estimates without introducing bias. The advantage function uses a baseline (the value function). GRPO uses group statistics as a baseline.
What is the policy gradient theorem?	The policy gradient theorem provides a formula for computing gradients of expected reward with respect to policy parameters, enabling direct policy optimization without needing a model of the environment. Foundation of algorithms like REINFORCE, PPO, GRPO.
What is sample efficiency in RL?	Sample efficiency measures how many environment interactions (data samples) are needed to learn a good policy. Sample-efficient algorithms learn faster from less data. GRPO is more sample-efficient than PPO for some tasks due to better advantage estimates.
What is the credit assignment problem?	The challenge of determining which actions in a sequence were responsible for the final reward. If a 100-step episode gets reward 1, which steps were good? Temporal difference learning and advantage functions help solve this.
What are partial rewards vs binary rewards?	Binary: 0 for failure, 1 for success - sparse, hard to learn from. Partial: graduated rewards for intermediate progress (0.25 for step 1, 0.6 for step 2, 1.0 for completion) - provides denser learning signal.
What is reward engineering?	The process of designing reward functions that accurately reflect the task goals while being learnable. Good reward engineering provides informative gradients, prevents hacking, and shapes learning toward desired behaviors.
Why is model evaluation important during training?	Regular evaluation on held-out data: 1) Detects overfitting early, 2) Measures actual task performance vs proxy metrics, 3) Catches mode collapse or reward hacking, 4) Informs when to stop training or adjust hyperparameters.
What is gradient variance and why does it matter?	Gradient variance measures how much gradient estimates fluctuate between batches. High variance = noisy gradients = unstable learning. Reducing variance (via baselines, larger batches, advantage functions) leads to more stable, efficient training.
What is the bias-variance tradeoff in RL?	Bias: systematic error in estimates (e.g., inaccurate value function). Variance: random fluctuation in estimates (e.g., noisy gradients). Low-bias methods (Monte Carlo) have high variance. Low-variance methods (TD learning) have bias. Must balance both.
What does "training stability" mean?	Stable training means loss decreases smoothly without wild fluctuations, gradients stay in reasonable ranges, and metrics improve consistently. Unstable training shows erratic loss, gradient explosions, or sudden performance collapses. Stability is crucial for reliable convergence.
What is a reference model/policy in RLHF/RLVF?	The reference model is a frozen copy of the initial policy before RL training. It's used to compute KL divergence penalties, preventing the trained policy from diverging too far from the initial behavior and maintaining coherent outputs.
What is early stopping?	Early stopping halts training when validation performance stops improving (or starts degrading) even though training loss keeps decreasing. Prevents overfitting and saves computation. Requires monitoring validation metrics during training.
What is the difference between online and offline RL?	Online RL: agent learns while interacting with environment, constantly collecting new data with updated policy. Offline RL: agent learns from fixed dataset collected beforehand, no environment interaction during training. Online is more sample-efficient but requires environment access.
What is multi-task learning in the context of RL?	Training a single model to perform multiple tasks simultaneously. Benefits: shared representations, better generalization, knowledge transfer between tasks. Challenge: balancing tasks, preventing catastrophic forgetting. Requires careful curriculum and task mixing.
Why might you use curriculum learning in RL?	Curriculum learning starts with easy problems and gradually increases difficulty. Benefits: faster initial learning, better final performance, avoids getting stuck on hard problems early. Mimics how humans learn - master basics before advanced concepts.

## CLOZE DELETION CARDS (use {{c1::text}} format)
In reinforcement learning, the {{c1::policy}} defines how an agent chooses actions, the {{c2::value function}} estimates expected rewards from states, and the {{c3::advantage function}} measures how much better an action is than average.
GRPO stands for {{c1::Group Relative Policy Optimization}}, which computes advantages using {{c2::relative comparisons within groups}} instead of requiring a {{c3::value function or critic network}}.
LoRA stands for {{c1::Low-Rank Adaptation}}, a {{c2::parameter-efficient}} fine-tuning method that adds small {{c3::trainable adapter matrices}} to frozen base model weights.
The three main components of a typical RLHF pipeline are: {{c1::Supervised Fine-Tuning (SFT)}} to teach basic task structure, {{c2::Reinforcement Learning}} to optimize for rewards, and {{c3::evaluation}} on held-out data.
Mode collapse warning signs include: entropy {{c1::<0.10}}, frac_reward_zero_std {{c2::>0.7}}, and {{c3::reward plateaus with decreasing diversity}}.
The exploration-exploitation tradeoff balances {{c1::trying new actions to discover better solutions}} (exploration) with {{c2::using known good actions to maximize reward}} (exploitation).
In policy gradient methods, using {{c1::advantage functions}} instead of raw rewards reduces {{c2::variance}} in gradient estimates, leading to more {{c3::stable training}}.
KL divergence measures {{c1::how different two probability distributions are}} and is used in RL to {{c2::prevent the updated policy from diverging too far}} from the {{c3::reference policy}}.
Reward shaping provides {{c1::partial credit}} for intermediate progress, creating {{c2::dense rewards}} that are easier to learn from than {{c3::sparse binary rewards}}.
The learning rate controls {{c1::step size}} when updating parameters. Too high causes {{c2::instability and overshooting}}, while too low causes {{c3::slow learning or getting stuck}}.
Gradient clipping prevents {{c1::gradient explosion}} by {{c2::limiting the magnitude of gradients}} to a threshold, which {{c3::stabilizes training without changing gradient direction}}.
Overfitting occurs when a model {{c1::learns training data too well including noise}}, causing {{c2::poor generalization}} to new data. Detected by {{c3::train/validation metric divergence}}.
Entropy regularization adds an {{c1::entropy bonus}} to rewards to encourage {{c2::diverse outputs}} and prevent {{c3::premature convergence or mode collapse}}.
RLVF (Reinforcement Learning from {{c1::Verifiable}} Feedback) uses {{c2::automated verification systems}} like compilers or test suites instead of {{c3::human feedback}}, making it infinitely scalable.
The discount factor gamma (γ) determines how much the agent values {{c1::future rewards}} versus {{c2::immediate rewards}}. γ=0 means only {{c3::immediate rewards matter}}, while γ=1 means all future rewards are equally important.
Completion-only training masks {{c1::prompt tokens}} from the loss, so the model only learns to {{c2::generate good completions}} rather than {{c3::memorize prompts}}.
