# Reinforcement Learning & Training Concepts Anki Deck
# Import Instructions: In Anki, go to File > Import, select this file, choose "Basic" or "Cloze" type based on card

## BASIC CARDS (Front	Back format)
What is a policy in reinforcement learning?	A policy is a mapping from states to actions that defines how an agent behaves. In language models, it's the probability distribution over possible next tokens given the current context.
What is the difference between on-policy and off-policy RL?	On-policy methods learn about the policy they're currently executing (e.g., SARSA, GRPO). Off-policy methods learn about a different policy than the one generating data (e.g., Q-learning). On-policy is more stable but less sample-efficient.
What is an advantage function in RL?	The advantage function A(s,a) measures how much better an action is compared to the average action in that state. Formally: A(s,a) = Q(s,a) - V(s), where Q is action-value and V is state-value. Positive advantage means better than average.
Why use advantage instead of raw rewards in policy gradient methods?	Using advantages reduces variance in gradient estimates. It tells the model "this action was X better/worse than average" rather than "this action got reward R" (which could be high even if the action was bad). This leads to more stable training.
What is the key innovation of GRPO (Group Relative Policy Optimization)?	GRPO eliminates the need for a value function/critic by computing advantages using relative comparisons within a group of solutions to the same problem. It compares multiple completions for each prompt and uses group statistics to estimate advantages.
Compare GRPO vs PPO: Which requires a critic network?	PPO requires a critic network (value function) to estimate baselines for advantage calculation. GRPO does not - it uses relative comparisons within groups, making it simpler and more stable for some tasks.
What is mode collapse in neural network training?	Mode collapse occurs when a model converges to generating repetitive or degenerate outputs instead of diverse solutions. The model "collapses" to a narrow region of the output space, often exploiting a loophole in the reward function.
What are three warning signs of mode collapse?	1) Low entropy (< 0.10) - model outputs become deterministic<br>2) High frac_zero_std - most samples get identical rewards<br>3) Reward plateaus while diversity drops - model found a "hack"
Why is entropy important in RL training?	Entropy measures the randomness/diversity of the policy's outputs. High entropy = exploring many options. Low entropy = exploitation/convergence. Without enough entropy, the model may collapse to repetitive outputs or fail to explore better solutions.
What is entropy regularization and why use it?	Adding an entropy bonus to the reward encourages the model to maintain diverse outputs during training. It prevents premature convergence to a single solution and helps exploration. However, too much can destabilize training.
What is KL divergence and why is it used in policy optimization?	KL divergence measures how different two probability distributions are. In RL, it's used to penalize the updated policy from diverging too far from the reference policy, preventing destructive updates that could collapse performance.
What does the "beta" parameter control in PPO/GRPO?	Beta is the coefficient for the KL divergence penalty. Higher beta = stronger constraint keeping the new policy close to the old one. It's a stability vs speed tradeoff: high beta = stable but slow learning, low beta = fast but risky.
What is the difference between sparse and dense rewards?	Sparse rewards: feedback only at final outcome (e.g., 1 if test passes, 0 otherwise). Dense rewards: feedback at every step (e.g., partial credit for each correct intermediate step). Dense rewards are easier to learn from but harder to design.
What is reward shaping?	Reward shaping modifies the reward function to provide more informative feedback while preserving the optimal policy. Example: instead of 0/1 for fail/pass, use graduated rewards like 0.25 for type-checking, 0.60 for compilation, 1.0 for passing tests.
What is reward hacking and how does it happen?	Reward hacking is when a model finds unintended ways to maximize reward without solving the actual task. Example: generating repetitive tokens that happen to pass a simple regex check, or exploiting edge cases in the reward function.
How can you prevent reward hacking?	1) Use multi-signal detection (require multiple independent checks)<br>2) Design robust reward functions (graduated, not binary)<br>3) Monitor for degenerate outputs<br>4) Use verifiable feedback when possible<br>5) Regular human evaluation
What is the gradient in gradient descent?	The gradient is the vector of partial derivatives of the loss with respect to each parameter. It points in the direction of steepest increase of the loss. We move in the opposite direction (negative gradient) to minimize loss.
What does learning rate control?	Learning rate controls the step size when updating parameters. Too high = unstable training, overshooting minima. Too low = slow training, may get stuck. It's the most important hyperparameter to tune.
What is gradient explosion and how to detect it?	Gradient explosion is when gradients become extremely large, causing parameter updates to overshoot and destabilize training. Detected by: grad_norm > 10, loss becoming NaN, or parameters diverging. Solution: gradient clipping or lower learning rate.
What is gradient vanishing and how to detect it?	Gradient vanishing is when gradients become extremely small, causing learning to stop. Detected by: grad_norm near 0, loss not decreasing, parameters not updating. Common in deep networks. Solutions: better initialization, normalization, or architecture changes.
What is gradient clipping?	Gradient clipping limits the magnitude of gradients to prevent explosion. If gradient norm exceeds a threshold, scale it down proportionally. This stabilizes training without changing the gradient direction.
What is a loss function?	A loss function (or objective function) quantifies how badly the model is performing. It's a single number we want to minimize. For RL: often negative expected reward. For supervised learning: difference between predictions and targets.
Why might loss be negative in GRPO?	In policy gradient methods, loss = -advantage, meaning we're maximizing advantages. Negative loss means the model is being pushed toward actions with positive advantages (better than average). More negative = stronger push toward good actions.
What does "overfitting" mean?	Overfitting is when a model learns the training data too well, including noise and specifics, failing to generalize to new data. Signs: training loss keeps decreasing while validation loss increases.
How can you detect overfitting?	1) Monitor train vs validation metrics - divergence indicates overfitting<br>2) Model performs well on training data but poorly on test data<br>3) Validation loss starts increasing while training loss decreases
What is the train/validation/test split for?	Training set: used to update model parameters. Validation set: used to tune hyperparameters and detect overfitting. Test set: final evaluation of model performance, never seen during development. Prevents "overfitting to the validation set."
What is LoRA (Low-Rank Adaptation)?	LoRA is a parameter-efficient fine-tuning method that freezes the original model weights and adds small trainable "adapter" matrices. Instead of updating millions of parameters, only update thousands in low-rank matrices that get added to the original weights.
Why use LoRA instead of full fine-tuning?	1) Memory efficient - only store adapter weights<br>2) Faster training - fewer parameters to update<br>3) Easier to distribute - share small adapter files instead of full model<br>4) Less prone to catastrophic forgetting<br>5) Can train multiple adapters for different tasks
What does "rank" mean in LoRA?	The rank determines the size of the adapter matrices. Higher rank = more parameters = more expressive but more memory. Typical values: 8-64. Low rank exploits the idea that weight updates lie in a low-dimensional subspace.
What is Supervised Fine-Tuning (SFT)?	SFT is training a pre-trained model on labeled input-output pairs using supervised learning (cross-entropy loss). In LLM fine-tuning, it's often used as initialization before reinforcement learning to teach basic task structure.
Why do SFT before reinforcement learning?	SFT provides a good initialization by teaching the model basic task structure and syntax. Starting RL from a random or base model is inefficient and unstable. SFT → RL is the standard pipeline for RLHF/RLVR.
What is RLVF (Reinforcement Learning from Verifiable Feedback)?	RLVF uses automated verification systems (compilers, test suites, formal provers) instead of human feedback to generate rewards. It's a subset of RLHF that's infinitely scalable since verification is automated.
Compare RLHF vs RLVF	RLHF (Human Feedback): humans provide rewards, expensive and slow to scale, subjective. RLVF (Verifiable Feedback): automated systems (compilers, tests) provide rewards, free and scalable, objective. RLVF only works when verification is possible.
What is an epoch in training?	One epoch is a complete pass through the entire training dataset. If you have 1000 examples and process them all once, that's 1 epoch. Training typically runs for multiple epochs (10, 100, etc.).
What is batch size?	Batch size is the number of training examples processed together before updating parameters. Larger batches = more stable gradients but more memory. Smaller batches = noisier gradients but better generalization. Typical: 8-128.
What is gradient accumulation?	Gradient accumulation simulates larger batch sizes by accumulating gradients over multiple small batches before updating parameters. Useful when GPU memory limits batch size. Accumulating over 4 batches = 4x effective batch size.
What does convergence mean in training?	Convergence means the model has reached a stable point where further training doesn't significantly improve the loss. The loss curve flattens out. Note: converging doesn't always mean good performance - you might converge to a bad solution!
What is a learning rate schedule?	A learning rate schedule changes the learning rate during training. Common: start high for fast learning, then decay to fine-tune. Example: linear decay, cosine annealing, step decay. Prevents overshooting near convergence.
What is the exploration-exploitation tradeoff?	Exploration: trying new actions to discover better solutions. Exploitation: using known good actions to maximize reward. Too much exploration = never optimize what works. Too much exploitation = miss better solutions. Must balance both.
What is a value function in RL?	A value function V(s) estimates the expected cumulative reward from a state following the current policy. It answers "how good is this state?" Used in many RL algorithms to guide learning, but not needed in GRPO.
What is a Q-function (action-value function)?	Q(s,a) estimates the expected cumulative reward from taking action a in state s, then following the policy. It answers "how good is this action in this state?" Used in Q-learning, actor-critic methods.
What is temporal difference (TD) learning?	TD learning updates value estimates based on differences between predicted and actual rewards plus next-state value. It learns from incomplete episodes by bootstrapping from current estimates. Example: V(s) ← V(s) + α[r + γV(s') - V(s)]
What is the discount factor (gamma) in RL?	Gamma (γ) determines how much the agent values future rewards vs immediate rewards. γ=0: only care about immediate reward. γ=1: all future rewards equally important. Typical: 0.9-0.99. Balances short-term and long-term planning.
What is on-policy vs off-policy learning?	On-policy: learn about the policy you're currently using to collect data (e.g., SARSA, GRPO). Off-policy: learn about a target policy while collecting data with a different behavior policy (e.g., Q-learning, DQN). Off-policy is more sample-efficient but less stable.
What is the curse of dimensionality in RL?	As state/action spaces grow, the number of possible states grows exponentially, making it impossible to visit or learn about all states. This is why function approximation (neural networks) is needed for complex tasks.
What is reward_std and why monitor it?	Reward_std (reward standard deviation) measures the diversity of rewards across a batch. High std = model generates solutions with varied performance. Low std = all solutions getting similar rewards, possible mode collapse warning.
What does frac_reward_zero_std mean?	Fraction of problems where all generated solutions received identical rewards (std=0). High values (>0.7) are critical - it means the model is generating identical outputs for different prompts, strong sign of mode collapse.
What is catastrophic forgetting?	Catastrophic forgetting is when a neural network forgets previously learned tasks when trained on new tasks. The new task's gradients overwrite weights important for old tasks. Common in continual learning and fine-tuning.
How can you prevent catastrophic forgetting in fine-tuning?	1) Use parameter-efficient methods like LoRA (freeze base model)<br>2) Mix in data from previous tasks (rehearsal)<br>3) Use regularization to keep weights close to original (KL penalty)<br>4) Lower learning rates for fine-tuning
What is completion-only training?	Training where the loss is only computed on the completion/output tokens, not the prompt/input tokens. The model only learns to generate good outputs, not to memorize prompts. Common in instruction fine-tuning.
Why mask prompt tokens from loss in supervised fine-tuning?	If prompts are included in loss, the model wastes capacity memorizing prompts instead of learning to generate good completions. Masking focuses learning on the actual task: generating appropriate outputs.
What is a baseline in policy gradient methods?	A baseline is a reference value subtracted from rewards to reduce variance in gradient estimates without introducing bias. The advantage function uses a baseline (the value function). GRPO uses group statistics as a baseline.
What is the policy gradient theorem?	The policy gradient theorem provides a formula for computing gradients of expected reward with respect to policy parameters, enabling direct policy optimization without needing a model of the environment. Foundation of algorithms like REINFORCE, PPO, GRPO.
What is sample efficiency in RL?	Sample efficiency measures how many environment interactions (data samples) are needed to learn a good policy. Sample-efficient algorithms learn faster from less data. GRPO is more sample-efficient than PPO for some tasks due to better advantage estimates.
What is the credit assignment problem?	The challenge of determining which actions in a sequence were responsible for the final reward. If a 100-step episode gets reward 1, which steps were good? Temporal difference learning and advantage functions help solve this.
What are partial rewards vs binary rewards?	Binary: 0 for failure, 1 for success - sparse, hard to learn from. Partial: graduated rewards for intermediate progress (0.25 for step 1, 0.6 for step 2, 1.0 for completion) - provides denser learning signal.
What is reward engineering?	The process of designing reward functions that accurately reflect the task goals while being learnable. Good reward engineering provides informative gradients, prevents hacking, and shapes learning toward desired behaviors.
Why is model evaluation important during training?	Regular evaluation on held-out data: 1) Detects overfitting early, 2) Measures actual task performance vs proxy metrics, 3) Catches mode collapse or reward hacking, 4) Informs when to stop training or adjust hyperparameters.
What is gradient variance and why does it matter?	Gradient variance measures how much gradient estimates fluctuate between batches. High variance = noisy gradients = unstable learning. Reducing variance (via baselines, larger batches, advantage functions) leads to more stable, efficient training.
What is the bias-variance tradeoff in RL?	Bias: systematic error in estimates (e.g., inaccurate value function). Variance: random fluctuation in estimates (e.g., noisy gradients). Low-bias methods (Monte Carlo) have high variance. Low-variance methods (TD learning) have bias. Must balance both.
What does "training stability" mean?	Stable training means loss decreases smoothly without wild fluctuations, gradients stay in reasonable ranges, and metrics improve consistently. Unstable training shows erratic loss, gradient explosions, or sudden performance collapses. Stability is crucial for reliable convergence.
What is a reference model/policy in RLHF/RLVF?	The reference model is a frozen copy of the initial policy before RL training. It's used to compute KL divergence penalties, preventing the trained policy from diverging too far from the initial behavior and maintaining coherent outputs.
What is early stopping?	Early stopping halts training when validation performance stops improving (or starts degrading) even though training loss keeps decreasing. Prevents overfitting and saves computation. Requires monitoring validation metrics during training.
What is the difference between online and offline RL?	Online RL: agent learns while interacting with environment, constantly collecting new data with updated policy. Offline RL: agent learns from fixed dataset collected beforehand, no environment interaction during training. Online is more sample-efficient but requires environment access.
What is multi-task learning in the context of RL?	Training a single model to perform multiple tasks simultaneously. Benefits: shared representations, better generalization, knowledge transfer between tasks. Challenge: balancing tasks, preventing catastrophic forgetting. Requires careful curriculum and task mixing.
Why might you use curriculum learning in RL?	Curriculum learning starts with easy problems and gradually increases difficulty. Benefits: faster initial learning, better final performance, avoids getting stuck on hard problems early. Mimics how humans learn - master basics before advanced concepts.
What is the actor-critic architecture?	Actor-critic combines policy-based (actor) and value-based (critic) methods. The actor updates the policy, while the critic evaluates actions by estimating the value function. The critic's estimates help reduce variance in the actor's gradient updates.
What is the difference between Monte Carlo and TD methods for return estimation?	Monte Carlo waits until episode end to compute actual returns (low bias, high variance). TD estimates returns using current value estimates (biased, low variance). TD can learn from incomplete episodes and is more sample-efficient.
What is GAE (Generalized Advantage Estimation)?	GAE is a method for computing advantages that balances bias and variance using an exponentially-weighted average of n-step advantages. Controlled by lambda parameter: λ=0 gives TD(1-step), λ=1 gives Monte Carlo. Reduces variance while maintaining low bias.
What is n-step return in RL?	N-step return uses actual rewards for n steps, then bootstraps from value estimate. It's a middle ground between 1-step TD (high bias, low variance) and Monte Carlo (low bias, high variance). Balances learning speed and accuracy.
What is importance sampling in RL?	Importance sampling allows learning from data generated by a different policy (behavior policy) than the one being optimized (target policy). Uses probability ratios to correct for distribution mismatch. Critical for off-policy learning.
What is experience replay?	Experience replay stores past experiences (state, action, reward, next state) in a buffer and randomly samples mini-batches for training. Breaks temporal correlations, improves sample efficiency, and enables off-policy learning. Used in DQN and other algorithms.
What is a target network?	A target network is a frozen copy of the Q-network updated periodically (not every step). It provides stable targets for TD updates, preventing the "chasing a moving target" problem that destabilizes training. Used in DQN and related algorithms.
What is double Q-learning and why use it?	Double Q-learning uses two Q-networks to decorrelate action selection from value estimation, reducing overestimation bias. One network selects the best action, the other evaluates it. Leads to more accurate value estimates and better policies.
Compare model-free vs model-based RL	Model-free: learns policy/value directly from experience without modeling environment dynamics. Sample-inefficient but simpler. Model-based: learns environment model, uses it for planning. Sample-efficient but requires accurate modeling. Most practical RL is model-free.
What is the difference between deterministic and stochastic policies?	Deterministic policy: always chooses same action for a given state (a = π(s)). Stochastic policy: samples actions from probability distribution (a ~ π(·|s)). Stochastic policies enable exploration and are differentiable for gradient-based optimization.
What is SGD (Stochastic Gradient Descent)?	SGD updates parameters using gradients from individual samples or mini-batches instead of full dataset. "Stochastic" means using random samples. Faster than batch gradient descent, introduces noise that can help escape local minima, but noisier convergence.
What is momentum in optimization?	Momentum accumulates a moving average of past gradients to accelerate learning. It builds velocity in consistent gradient directions while dampening oscillations. Helps escape plateaus and speeds convergence. Update: v = βv + ∇L, θ = θ - αv. Typical β: 0.9.
What is the Adam optimizer?	Adam (Adaptive Moment Estimation) combines momentum (first moment) and RMSprop (second moment) with bias correction. Adapts learning rates per parameter based on gradient history. Most popular optimizer for deep learning. Requires less learning rate tuning than SGD.
Compare SGD vs Adam optimizer	SGD: simple, requires careful learning rate tuning, better generalization in some cases. Adam: adaptive learning rates, works well with default hyperparameters, faster convergence, but may generalize slightly worse. Adam is more popular for its ease of use.
What is RMSprop?	RMSprop (Root Mean Square Propagation) adapts learning rates by dividing by a moving average of squared gradients. Prevents learning rates from becoming too small or large. Good for non-stationary objectives and recurrent networks. Precursor to Adam.
What is learning rate warmup?	Learning rate warmup gradually increases the learning rate from near-zero to target value over initial training steps. Prevents instability from large updates when parameters are randomly initialized. Common in transformer training (e.g., 1000-10000 warmup steps).
What is cosine annealing for learning rates?	Cosine annealing varies learning rate following a cosine curve from max to min over training. Provides smooth decay with periodic restarts option. Often works better than linear decay. Popular schedule: lr(t) = lr_min + 0.5(lr_max - lr_min)(1 + cos(πt/T)).
What is weight decay and how does it differ from L2 regularization?	Both add penalty proportional to weight magnitude. Weight decay directly shrinks weights each step: θ = θ(1-λ) - α∇L. L2 regularization adds λ||θ||² to loss. They're equivalent for SGD but differ for adaptive optimizers like Adam. Weight decay is generally more effective.
What is L1 regularization and when to use it?	L1 regularization adds λ|θ| penalty to loss, encouraging sparse weights (many exactly zero). Useful for feature selection and interpretability. L1 creates sparse models, L2 creates small but non-zero weights. L1 is less smooth to optimize.
What is dropout and how does it work?	Dropout randomly sets a fraction of neuron activations to zero during training (typically 50%). This prevents co-adaptation of neurons, forcing the network to learn redundant representations. Acts as ensemble of sub-networks. Disabled during inference.
Why is dropout turned off during inference?	Dropout is a training-time regularization technique. During inference, we want the full model's predictions, not random subsets. Activations are scaled by the keep probability (or dropout is scaled during training) to maintain expected values.
What is batch normalization?	Batch normalization normalizes layer inputs across the batch dimension to have mean=0 and variance=1, then applies learned scale and shift. Reduces internal covariate shift, allows higher learning rates, acts as regularization. Computes batch statistics during training, uses running averages during inference.
What is layer normalization and how does it differ from batch normalization?	Layer norm normalizes across features for each sample independently (not across batch). More stable for small batches and sequences. Used in transformers. Batch norm: normalize across batch per feature. Layer norm: normalize across features per sample.
Why is weight initialization important?	Poor initialization can cause gradients to vanish/explode, making training impossible. Good initialization (like Xavier, He) sets weights to scale that maintains gradient magnitudes through layers. Affects training speed, convergence, and whether training succeeds at all.
What is Xavier/Glorot initialization?	Xavier initialization sets weights from distribution with variance 2/(n_in + n_out), where n_in and n_out are layer input/output dimensions. Designed to maintain gradient variance through layers with tanh/sigmoid activations. Prevents vanishing/exploding gradients.
What is He initialization?	He initialization sets weights with variance 2/n_in. Designed for ReLU activations, which kill negative values and need larger initial weights than tanh/sigmoid. Used in most modern deep networks with ReLU-family activations.
What is the dead ReLU problem?	Dead ReLU occurs when a neuron always outputs zero (negative inputs with ReLU). This stops gradient flow, making the neuron permanently inactive. Caused by large negative bias or learning rate too high. Solutions: Leaky ReLU, better initialization, lower learning rate.
What is temperature in sampling?	Temperature τ controls randomness when sampling from probability distributions. Higher τ = more uniform (random), lower τ = more peaked (deterministic). Logits are divided by τ before softmax. τ=1 is standard, τ→0 approaches argmax, τ→∞ approaches uniform.
What is top-k sampling?	Top-k sampling restricts sampling to the k most probable tokens, setting others to zero probability. Prevents sampling very unlikely tokens while maintaining diversity. Example: k=50 means sample from 50 most likely tokens. Balances quality and diversity.
What is nucleus (top-p) sampling?	Nucleus sampling (top-p) selects the smallest set of tokens whose cumulative probability ≥ p, then samples from this set. Adapts to probability distribution shape. p=0.9 is common. More flexible than top-k since set size varies with confidence.
Compare greedy decoding vs beam search	Greedy: always picks highest probability token. Fast, deterministic, but can miss better sequences. Beam search: maintains k best partial sequences, explores multiple paths. Slower but finds better solutions. Used when quality matters more than speed.
What is the cross-entropy loss?	Cross-entropy measures the difference between predicted probability distribution and true distribution. For classification: -log(p_correct_class). Minimizing cross-entropy maximizes likelihood of correct predictions. Standard loss for classification and language modeling.
What is perplexity and how does it relate to cross-entropy?	Perplexity = exp(cross_entropy). It represents the effective vocabulary size or "how confused" the model is. Lower perplexity = better language model. Perplexity of 100 means on average, the model is as uncertain as if randomly choosing from 100 tokens.
What is reward normalization in RL?	Reward normalization rescales rewards to have mean 0 and standard deviation 1 across the batch or recent history. Helps optimization by keeping rewards in consistent range, especially when reward scales vary. Can improve training stability.
What is observation/state normalization?	Normalizing inputs to have mean 0 and variance 1 helps neural networks learn more effectively. Gradient descent converges faster with normalized inputs. Often done using running statistics (mean, std) collected during training.
What is advantage normalization?	Normalizing advantages to mean 0, std 1 within each batch makes policy gradient updates more stable. Prevents single high-advantage sample from dominating the update. Standard practice in PPO and related algorithms.
What is the clipped surrogate objective in PPO?	PPO clips the probability ratio r = π_new/π_old to [1-ε, 1+ε] (typically ε=0.2) to limit policy update magnitude. This prevents destructive updates from large ratio values. Takes minimum of clipped and unclipped objectives for conservative updates.
What is a trust region in policy optimization?	A trust region constrains policy updates to a region where approximations are valid. Ensures new policy isn't too different from old one. PPO uses clipping, TRPO uses explicit KL constraint. Improves training stability by preventing large, destructive updates.
What is REINFORCE algorithm?	REINFORCE is the foundational policy gradient algorithm. It samples trajectories, computes returns, and updates policy to increase probability of high-return actions. Unbiased but high variance. Usually enhanced with baselines (value function) to reduce variance.
What are eligibility traces in RL?	Eligibility traces assign credit to states/actions based on how recently and frequently they occurred. They enable learning from TD errors across multiple time steps. Controlled by λ parameter. Unify TD and Monte Carlo methods (TD(λ)).
What is return in RL?	Return (or cumulative reward) is the total discounted reward from a time step onward: G_t = r_t + γr_{t+1} + γ²r_{t+2} + ... The goal of RL is to maximize expected return. Can be estimated (bootstrapped) or computed from full episode.
Compare value-based vs policy-based RL methods	Value-based: learn value/Q-function, derive policy (e.g., Q-learning, DQN). Works for discrete actions, more sample-efficient. Policy-based: directly learn policy (e.g., REINFORCE, PPO). Works for continuous actions, better for stochastic policies. Actor-critic combines both.
What is off-policy correction?	Off-policy correction adjusts learning when data comes from different policy than target policy. Uses importance sampling ratios or other techniques to account for distribution mismatch. Critical for experience replay and learning from demonstrations.
What is prioritized experience replay?	Prioritized ER samples experiences with probability proportional to TD error (or other priority metric). High-error experiences are replayed more often, improving sample efficiency. Requires importance sampling correction to remain unbiased.
What is mixed precision training?	Mixed precision uses both 16-bit (faster, less memory) and 32-bit (more precise) floating point. Most operations in FP16, some in FP32. Requires loss scaling to prevent gradient underflow. Speeds up training and reduces memory ~2x with minimal accuracy loss.
What is gradient checkpointing?	Gradient checkpointing trades computation for memory by not storing all intermediate activations. During backward pass, recomputes activations from checkpoints. Enables training larger models or batch sizes at cost of ~30% slower training.
What is data parallelism?	Data parallelism splits batches across multiple GPUs. Each GPU has full model copy, processes different data, then gradients are averaged. Scales to many GPUs for large batches. Most common parallelism strategy for distributed training.
What is model parallelism?	Model parallelism splits the model across multiple GPUs when it's too large for one GPU. Different GPUs compute different layers or parts of layers. More complex than data parallelism, used for very large models. Pipeline parallelism is a variant.
What is checkpoint averaging (model averaging)?	Averaging parameters from multiple checkpoints during training can improve generalization. Methods: average last K checkpoints, exponential moving average of parameters. Smooths out noise and often improves final performance at no extra training cost.
What is an ensemble in machine learning?	An ensemble combines predictions from multiple models to improve accuracy and robustness. Methods: averaging, voting, stacking. Generally more accurate than single models but requires training and storing multiple models. Dropout approximates ensembles.
What is bootstrapping in statistics/ML?	Bootstrapping creates multiple datasets by resampling with replacement from original data. Used to estimate uncertainty, train ensemble models, or assess model stability. In RL, also refers to using value estimates instead of full returns (TD learning).
What does "non-stationary" mean in the context of RL training?	Non-stationary means the data distribution changes over time. In RL, the policy changes during training, so the data distribution shifts. This violates supervised learning assumptions and makes RL harder. Techniques like target networks and experience replay help.
What is sample efficiency vs computational efficiency?	Sample efficiency: how many environment interactions needed to learn. Computational efficiency: how much compute needed per interaction. Model-based RL is sample-efficient but computationally expensive. Simple model-free is opposite. Trade-offs depend on environment cost.
What is a learning curve?	A learning curve plots model performance (accuracy, loss, reward) vs training progress (epochs, steps, samples). Used to diagnose training: good convergence, overfitting, underfitting, need for more data/capacity. Essential tool for understanding training dynamics.
What is a plateau in training?	A plateau is when loss stops decreasing for extended period despite continued training. Can indicate: need for lower learning rate, need for better optimization, model capacity limits, or need for architecture changes. Learning rate schedulers often reduce LR on plateau.
What is momentum-based optimization and why does it help?	Momentum-based optimizers (SGD+momentum, Adam) accumulate gradients over time, building velocity. Benefits: faster convergence in consistent directions, dampens oscillations, helps escape shallow local minima, smoother optimization trajectory. Critical for training deep networks efficiently.

## CLOZE DELETION CARDS (use {{c1::text}} format)
In reinforcement learning, the {{c1::policy}} defines how an agent chooses actions, the {{c2::value function}} estimates expected rewards from states, and the {{c3::advantage function}} measures how much better an action is than average.
GRPO stands for {{c1::Group Relative Policy Optimization}}, which computes advantages using {{c2::relative comparisons within groups}} instead of requiring a {{c3::value function or critic network}}.
LoRA stands for {{c1::Low-Rank Adaptation}}, a {{c2::parameter-efficient}} fine-tuning method that adds small {{c3::trainable adapter matrices}} to frozen base model weights.
The three main components of a typical RLHF pipeline are: {{c1::Supervised Fine-Tuning (SFT)}} to teach basic task structure, {{c2::Reinforcement Learning}} to optimize for rewards, and {{c3::evaluation}} on held-out data.
Mode collapse warning signs include: entropy {{c1::<0.10}}, frac_reward_zero_std {{c2::>0.7}}, and {{c3::reward plateaus with decreasing diversity}}.
The exploration-exploitation tradeoff balances {{c1::trying new actions to discover better solutions}} (exploration) with {{c2::using known good actions to maximize reward}} (exploitation).
In policy gradient methods, using {{c1::advantage functions}} instead of raw rewards reduces {{c2::variance}} in gradient estimates, leading to more {{c3::stable training}}.
KL divergence measures {{c1::how different two probability distributions are}} and is used in RL to {{c2::prevent the updated policy from diverging too far}} from the {{c3::reference policy}}.
Reward shaping provides {{c1::partial credit}} for intermediate progress, creating {{c2::dense rewards}} that are easier to learn from than {{c3::sparse binary rewards}}.
The learning rate controls {{c1::step size}} when updating parameters. Too high causes {{c2::instability and overshooting}}, while too low causes {{c3::slow learning or getting stuck}}.
Gradient clipping prevents {{c1::gradient explosion}} by {{c2::limiting the magnitude of gradients}} to a threshold, which {{c3::stabilizes training without changing gradient direction}}.
Overfitting occurs when a model {{c1::learns training data too well including noise}}, causing {{c2::poor generalization}} to new data. Detected by {{c3::train/validation metric divergence}}.
Entropy regularization adds an {{c1::entropy bonus}} to rewards to encourage {{c2::diverse outputs}} and prevent {{c3::premature convergence or mode collapse}}.
RLVF (Reinforcement Learning from {{c1::Verifiable}} Feedback) uses {{c2::automated verification systems}} like compilers or test suites instead of {{c3::human feedback}}, making it infinitely scalable.
The discount factor gamma (γ) determines how much the agent values {{c1::future rewards}} versus {{c2::immediate rewards}}. γ=0 means only {{c3::immediate rewards matter}}, while γ=1 means all future rewards are equally important.
Completion-only training masks {{c1::prompt tokens}} from the loss, so the model only learns to {{c2::generate good completions}} rather than {{c3::memorize prompts}}.
The actor-critic architecture has two components: the {{c1::actor}} which updates the policy, and the {{c2::critic}} which estimates the {{c3::value function}} to reduce variance.
GAE ({{c1::Generalized Advantage Estimation}}) balances bias and variance using parameter {{c2::lambda (λ)}}, where λ=0 gives {{c3::TD(1-step)}} and λ=1 gives {{c3::Monte Carlo}}.
Monte Carlo methods have {{c1::low bias}} but {{c2::high variance}}, while TD learning has {{c3::some bias}} but {{c4::low variance}}.
The Adam optimizer combines {{c1::momentum}} (first moment) and {{c2::RMSprop}} (second moment) with {{c3::bias correction}}.
SGD with momentum accumulates velocity: {{c1::v = βv + ∇L}}, then updates parameters: {{c2::θ = θ - αv}}, where typical β is {{c3::0.9}}.
Batch normalization normalizes across the {{c1::batch dimension}}, while layer normalization normalizes across {{c2::features}} for each sample independently.
Weight initialization methods include {{c1::Xavier/Glorot}} for tanh/sigmoid activations and {{c2::He initialization}} for {{c3::ReLU}} activations.
Temperature sampling divides logits by {{c1::τ (tau)}}. Higher temperature = {{c2::more random}}, lower temperature = {{c3::more deterministic}}.
Top-k sampling restricts to the {{c1::k most probable tokens}}, while nucleus (top-p) sampling selects tokens with cumulative probability {{c2::≥ p}}.
Experience replay stores {{c1::past experiences}} in a buffer and {{c2::randomly samples}} mini-batches, which breaks {{c3::temporal correlations}} and enables {{c4::off-policy learning}}.
A target network is a {{c1::frozen copy}} of the Q-network updated {{c2::periodically}}, providing {{c3::stable targets}} for TD updates.
Double Q-learning uses {{c1::two Q-networks}} to reduce {{c2::overestimation bias}} by decorrelating {{c3::action selection}} from {{c4::value estimation}}.
Model-free RL learns {{c1::policy/value}} directly from experience, while model-based RL learns an {{c2::environment model}} for {{c3::planning}}.
Dropout sets a fraction of activations to {{c1::zero}} during {{c2::training}} but is {{c3::disabled}} during inference.
L1 regularization encourages {{c1::sparse weights}} (many exactly zero), while L2 regularization creates {{c2::small but non-zero weights}}.
The clipped surrogate objective in PPO clips the ratio r to {{c1::[1-ε, 1+ε]}} where ε is typically {{c2::0.2}}.
REINFORCE is the foundational {{c1::policy gradient}} algorithm. It has {{c2::low bias}} but {{c3::high variance}} and is usually enhanced with {{c4::baselines}}.
In importance sampling, we learn about the {{c1::target policy}} using data from the {{c2::behavior policy}} by using {{c3::probability ratios}}.
N-step returns balance between {{c1::1-step TD}} (high bias, low variance) and {{c2::Monte Carlo}} (low bias, high variance).
Mixed precision training uses {{c1::16-bit}} floats for speed and {{c2::32-bit}} floats for precision, requiring {{c3::loss scaling}} to prevent gradient underflow.
Data parallelism splits {{c1::batches}} across GPUs, while model parallelism splits the {{c2::model itself}} across GPUs.
Learning rate warmup gradually {{c1::increases}} the learning rate from {{c2::near-zero}} to prevent {{c3::instability}} from large updates with random initialization.
Cross-entropy loss for classification is {{c1::-log(p_correct_class)}}, and perplexity equals {{c2::exp(cross_entropy)}}.
Reward normalization rescales rewards to mean {{c1::0}} and standard deviation {{c2::1}}, improving {{c3::optimization stability}}.
The discount factor gamma (γ) balances {{c1::short-term}} vs {{c2::long-term}} rewards. γ=0 means only immediate rewards matter, γ=1 means all future rewards are equally important.
Value-based methods learn {{c1::value/Q-functions}} and work for {{c2::discrete actions}}, while policy-based methods {{c3::directly learn policies}} and work for {{c4::continuous actions}}.
Off-policy correction uses {{c1::importance sampling}} or other techniques to account for {{c2::distribution mismatch}} when learning from different policies.
Gradient clipping limits gradient magnitude by {{c1::scaling down}} gradients that exceed a {{c2::threshold}}, preventing {{c3::gradient explosion}}.
Prioritized experience replay samples experiences proportional to {{c1::TD error}}, requiring {{c2::importance sampling correction}} to remain unbiased.
Non-stationary training means the {{c1::data distribution changes}} over time, violating {{c2::supervised learning assumptions}}.
Eligibility traces assign credit based on how {{c1::recently}} and {{c2::frequently}} states/actions occurred, controlled by {{c3::λ (lambda)}} parameter.
Trust regions constrain policy updates to ensure the new policy isn't {{c1::too different}} from the old one, improving {{c2::training stability}}.
Sample efficiency measures {{c1::how many environment interactions}} are needed to learn, while computational efficiency measures {{c2::compute per interaction}}.
A learning curve plots {{c1::model performance}} vs {{c2::training progress}} to diagnose convergence, overfitting, or underfitting.
Cosine annealing follows the formula: lr(t) = lr_min + {{c1::0.5}}(lr_max - lr_min)(1 + {{c2::cos(πt/T)}}).
The reference model in RLHF/RLVF is a {{c1::frozen copy}} of the initial policy used to compute {{c2::KL divergence penalties}}.
